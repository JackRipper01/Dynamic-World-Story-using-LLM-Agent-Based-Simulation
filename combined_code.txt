
============================== START FILE: src_GM\action_resolver.py ==============================

import google.generativeai as genai  # Keep LLM import here if resolver uses it
import json
import re
from abc import ABC, abstractmethod


class BaseActionResolver(ABC):
    """
    Abstract Base Class for interpreting an agent's intended action output
    and determining its outcome within the world state.
    Different implementations will represent different validation/resolution strategies.
    """
    @abstractmethod
    def resolve(self, agent_name: str, agent_location: str, action_output: str, world_state) -> dict:
        """
        Takes the agent's raw output and the current world state,
        determines the actual outcome, and returns a structured result.

        Args:
            agent_name: Name of the agent attempting the action.
            agent_location: Location of the agent when attempting the action.
            action_output: The raw output from the agent's thinking module.
            world_state: Reference to the current WorldState.

        Returns:
            A dictionary containing resolution details, e.g.:
            {
                "success": bool,
                "action_type": "MOVE" | "SPEAK" | "INTERACT" | "OBSERVE" | "WAIT" | "FAIL" | "UNKNOWN",
                "parameters": dict, # Action specific details derived by the resolver
                "reasoning": str, # Explanation (optional)
                "outcome_description": str, # What an observer sees happen
                # List of direct state changes, e.g., [('agent_location', agent_name, 'NewLoc'), ('lock_state', 'ShelterDoor', False)]
                "world_state_updates": list,
            }
            Return None or a dict with success=False if resolution fails severely.
        """
        pass


# --- Concrete Implementation (Moves logic from LLMInterpreter) ---


class LLMActionResolver(BaseActionResolver):
    """
    Uses an LLM to interpret natural language action output, validate it
    against world rules (potentially simplified), and generate an outcome.
    (Based on the original LLMInterpreter logic)
    """

    def __init__(self, model, world_state_ref_for_prompting_rules=None):
        self.llm = model
        # Might need a way to get *some* world info for the prompt,
        # but avoid passing the full mutable state if possible.
        # Maybe pass specific rule functions or data? For now, keep it simple.
        self.world_ref = world_state_ref_for_prompting_rules  # Use carefully

    def resolve(self, agent_name: str, agent_location: str, action_output: str, world_state) -> dict:
        print(
            f"[LLM Resolver @ {agent_location}]: Resolving for {agent_name}: '{action_output}'")

        # 1. Gather Context (Simplified example - adapt as needed)
        # This part needs careful design - what *minimal* context does the resolver need?
        # Avoid giving it the full dynamic event history if possible.
        # rules = f"Rules: Shelter door currently {world_state.get_location_property('Shelter', 'door_locked')}."
        connectivity = f"From {agent_location}, exits lead to: {world_state.get_reachable_locations(agent_location)}."
        agents_present = world_state.get_agents_at(agent_location)
        others_present = [
            name for name in agents_present if name != agent_name]
        state_summary = f"Others present: {others_present if others_present else 'None'}."

        # 2. Craft Prompt (Similar to old Interpreter prompt, but focused on resolution)
        prompt = f"""You are the Action Resolver for a simulation.
Agent '{agent_name}' at location '{agent_location}' intends to: "{action_output}"

Relevant world state and rules:
{connectivity}
{state_summary}
Weather: {world_state.global_context.get('weather', 'Clear')}

Analyze the agent's intent. Is it possible? What is the most plausible outcome?
Output a JSON object describing the outcome:
{{
    "success": true | false,
    "action_type": "MOVE | SPEAK | INTERACT | OBSERVE | WAIT | FAIL | UNKNOWN",
    "parameters": {{ // e.g., "destination": "X", "target": "Y", "message": "..." }},
    "reasoning": "Brief explanation.",
    "outcome_description": "Short sentence of what an observer sees.",
    "world_state_updates": [ // OPTIONAL: List of ['attribute', 'target', 'new_value'] tuples
        // e.g., ["agent_location", "{agent_name}", "Park"], ["location_property", "Shelter", "door_locked", false]
    ]
}}

Example (Move success): {{"success": true, "action_type": "MOVE", "parameters": {{"destination": "Park"}}, "reasoning": "Path is clear.", "outcome_description": "{agent_name} walks towards the Park.", "world_state_updates": [["agent_location", "{agent_name}", "Park"]] }}
Example (Move fail): {{"success": false, "action_type": "MOVE", "parameters": {{"destination": "Shelter"}}, "reasoning": "Door is locked.", "outcome_description": "{agent_name} tries the Shelter door, but it's locked.", "world_state_updates": [] }}
Example (Speak): {{"success": true, "action_type": "SPEAK", "parameters": {{"target": "Bob", "message": "Hello"}}, "reasoning": "Bob is present.", "outcome_description": "{agent_name} says to Bob, 'Hello'.", "world_state_updates": [] }}

Your JSON Output:
```json
"""

    # 3. Call LLM & Parse (Similar parsing logic as before)
        try:
            # Add JSON mode if available/needed
            response = self.llm.generate_content(
                prompt)  # Assuming self.llm is configured
            raw_output = response.text
            # Parse JSON (reuse robust parsing logic from your interpreter)
            parsed_json = self._parse_llm_output(raw_output)

            if parsed_json and 'success' in parsed_json and 'outcome_description' in parsed_json:
                # Basic validation passed
                # Ensure 'world_state_updates' is a list, default to empty if missing
                parsed_json['world_state_updates'] = parsed_json.get(
                    'world_state_updates', [])
                return parsed_json
            else:
                print(
                    f"[LLM Resolver Error]: Failed to parse or validate LLM output: {raw_output}")
                    # Return a generic failure dictionary
                return {
                        "success": False,
                    "action_type": "FAIL",
                    "parameters": {"raw_output": action_output},
                    "reasoning": "Failed to interpret action via LLM.",
                    "outcome_description": f"{agent_name} does something unclear or fails ('{action_output}').",
                    "world_state_updates": []
                    }

        except Exception as e:
            print(f"[LLM Resolver Error]: LLM call or processing failed: {e}")
            return {  # Generic failure dictionary
                "success": False, "action_type": "FAIL", "parameters": {"raw_output": action_output},
                "reasoning": f"LLM exception: {e}",
                "outcome_description": f"{agent_name}'s action ('{action_output}') causes confusion or fails.",
                "world_state_updates": []
            }

    # Add the _parse_llm_output helper method here (copied from your interpreter)
    def _parse_llm_output(self, raw_output):
        try:
            json_match = re.search(r'```json\s*([\s\S]+?)\s*```', raw_output)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = raw_output
            data = json.loads(json_str)
            # Add more validation if needed
            return data
        except json.JSONDecodeError as e:
            print(f"[LLM Resolver Error]: JSON Decode failed: {e}. Raw output: {raw_output}")
            return None
        except Exception as e:
            print(f"[LLM Resolver Error]: Unexpected parsing error: {e}")
            return None


============================== END FILE: src_GM\action_resolver.py ==============================


============================== START FILE: src_GM\config.py ==============================

# config.py
import os
from dotenv import load_dotenv

# Load API Key
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError(
        "Gemini API Key not found. Make sure it's set in the .env file.")

# LLM Generation Settings
GENERATION_CONFIG = {
    "temperature": 0.9,  # Director might benefit from slightly lower temp if too random
    "top_p": 0.95,
    "top_k": 50,
    "max_output_tokens": 150,
}

# LLM Safety Settings
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT",
        "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH",
        "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
]


# Model Name
MODEL_NAME = "gemini-1.5-flash"

# Simulation Settings
MAX_RECENT_EVENTS = 15
MAX_MEMORY_TOKENS = 600
SIMULATION_MAX_STEPS = 30

# World Settings
KNOWN_LOCATIONS = {
    "Park": "A wide open park area with some trees.",
    "Shelter": "A simple wooden shelter.",
    "Forest Edge": "The edge of a dark forest."
}

# Agent Settings
DEFAULT_PERSONALITIES = {
    "Alice": "curious, slightly anxious, observant",
    "Bob": "calm, pragmatic, speaks plainly"
}

# --- Component Selection ---
# Choose the implementations for different parts of the simulation
# (Allows easy switching for experiments)

# Options: "SimpleMemory", "VectorMemory" (future)
AGENT_MEMORY_TYPE = "SimpleMemory"
# Options: "GeminiThinker", "RuleBasedThinker" (future)
AGENT_THINKER_TYPE = "GeminiThinker"

# Action Resolution Strategy (Crucial for your experiments!)
# Options: "LLMResolver", "StructuredValidator", "Passthrough", "HybridRefine" (future)
ACTION_RESOLVER_TYPE = "LLMResolver"  # Start with the current LLM-based logic

# Event Perception Model (How agents receive events)
# Options: "DirectDispatch", "SummaryContext" (old way)
EVENT_PERCEPTION_MODEL = "DirectDispatch"

NARRATIVE_GOAL = "Develop a beautiful and funny story."
# DIRECTOR_COOLDOWN_MIN = 2 # Example: Could make cooldown configurable
# DIRECTOR_COOLDOWN_MAX = 5


============================== END FILE: src_GM\config.py ==============================


============================== START FILE: src_GM\director.py ==============================

# src/director.py
import google.generativeai as genai
import config
import random # For potential randomness or choosing not to act

class Director:
    """
    Observes the world state, considers a narrative goal, and uses an LLM
    to suggest and enact *indirect* environmental interventions.
    Inspired by the Game Master concept.
    """
    def __init__(self, world_state, model, narrative_goal):
        self.world = world_state # Reference to the world state
        self.llm = model         # The LLM instance for decision making
        self.narrative_goal = narrative_goal
        self.intervention_cooldown = 0 # Steps until next potential intervention
        self.steps_since_last_intervention = 0
        print(f"Director initialized with goal: '{self.narrative_goal}'")

    def observe(self):
        """Gathers the necessary information from the world state for the LLM."""
        # Get a concise summary of the current state
        agent_locations = self.world.agent_locations
        recent_events = self.world.event_log[-5:]  # Last 5 events
        weather = self.world.global_context.get('weather', 'unknown')

        observation = f"Current State Summary:\n"
        observation += f"- Weather: {weather}\n"
        observation += f"- Agent Locations: {agent_locations}\n"
        observation += f"- Recent Events:\n"
        if recent_events:
            for event in recent_events:
                # Check if the event object still has these attributes (it should)
                # Event = namedtuple("Event", ["description", "location", "scope", "step", "triggered_by"])
                location_str = event.location or 'Global'
                scope_str = event.scope or 'UnknownScope'
                triggered_by_str = event.triggered_by or 'UnknownTrigger'
                # Added check for attribute existence just in case, though they should be there
                desc_str = getattr(event, 'description', 'No description')

                # Updated prefix formatting for clarity and robustness
                prefix = f"[S{event.step} {triggered_by_str} @ {location_str}/{scope_str}]"
                observation += f"    - {prefix} {desc_str}\n"  # Use desc_str
        else:
            observation += "    - Nothing noteworthy recently in event log.\n"

        # Add goal progress check (simple example: are agents together?)
        agents_in_same_location = False
        if len(agent_locations) > 1:
            first_loc = next(iter(agent_locations.values()))
            if all(loc == first_loc for loc in agent_locations.values()):
                agents_in_same_location = True
        observation += f"- Goal Status ({self.narrative_goal}): Agents currently {'together' if agents_in_same_location else 'apart'}.\n"

        return observation.strip()

    def think(self, observation):
        """Uses the LLM to decide on an environmental intervention."""

        # Simple cooldown mechanism
        if self.steps_since_last_intervention < self.intervention_cooldown:
             self.steps_since_last_intervention += 1
             print("[Director Thinking]: Cooldown active.")
             return "ACTION: Do nothing" # Skip thinking

        prompt = f"""You are the Director/Game Master of a simulation.
Your objective is to subtly guide the narrative towards: '{self.narrative_goal}'.
You do this *only* by manipulating the environment or introducing external events.
You CANNOT directly control agents, read their minds, or change their internal states.

Current Simulation State:
{observation}

Allowed Actions:
1. Change weather: Specify the new weather condition (e.g., Sunny, Rainy, Windy, Foggy, Stormy).
2. Create ambient event: Describe a sensory event (e.g., a distant sound, a strange smell, a sudden chill).
3. Do nothing: If the situation doesn't require intervention or is progressing well.

Based on the current state and your objective, suggest ONE action from the allowed list to subtly nudge things towards the goal. Be concise and use the format 'ACTION: [Your chosen action description]'. If doing nothing, use 'ACTION: Do nothing'.

Example suggestions:
ACTION: Change weather to Rainy
ACTION: Create ambient event 'A faint birdsong is heard from the Forest Edge.'
ACTION: Do nothing

Your suggestion:"""

        print("[Director Thinking...]")
        # print(f"--- DEBUG PROMPT for Director ---\n{prompt}\n--------------------") # Optional

        try:
            response = self.llm.generate_content(prompt)
            suggestion = response.text.strip()

            if not suggestion.startswith("ACTION:"):
                print("[Director Warning]: LLM response malformed. Defaulting to 'Do nothing'. Response:", suggestion)
                return "ACTION: Do nothing"

            print(f"[Director Suggests]: {suggestion}")
            # Reset cooldown if an action is suggested (or consider resetting only on non-'Do nothing' actions)
            if "Do nothing" not in suggestion:
                 self.steps_since_last_intervention = 0
                 self.intervention_cooldown = random.randint(2, 4) # Wait a few steps after acting
                 print(f"[Director Info]: Intervention suggested. Setting cooldown to {self.intervention_cooldown} steps.")
            else:
                 self.steps_since_last_intervention += 1 # Increment even if thinking 'do nothing' this time
            return suggestion

        except Exception as e:
            print(f"[Director Error]: LLM generation failed: {e}")
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                 print(f"[Director Safety Block]: Reason: {response.prompt_feedback.block_reason}")
            return "ACTION: Do nothing" # Fail safe

    def act(self, suggestion):
        """Parses the LLM's suggestion and applies it to the world state."""
        action_part = suggestion.replace("ACTION:", "").strip()

        if action_part.lower() == "do nothing":
            print("[Director Action]: No intervention taken.")
            return

        # Parse specific actions
        if action_part.lower().startswith("change weather to "):
            new_weather = action_part[len("change weather to "):].strip().title()
            if new_weather:
                print(f"[Director Action]: Attempting to change weather to {new_weather}")
                self.world.set_weather(new_weather, triggered_by="Director")
            else:
                print("[Director Action Error]: Invalid weather condition specified.")

        elif action_part.lower().startswith("create ambient event "):
            event_description = action_part[len("create ambient event "):].strip()
            # Add quotes or formatting for clarity if needed
            if not event_description.startswith("'"): event_description = f"'{event_description}'"

            if event_description:
                scope = 'global'
                print(f"[Director Action]: Creating {scope} ambient event at '{'Global'}': {event_description}")
                self.world.log_event(f"[Ambient]: {event_description}",
                                       scope=scope,
                                       location='Global',
                                       triggered_by="Director")
            else:
                print("[Director Action Error]: Invalid ambient event description.")
        else:
            print(f"[Director Action Warning]: Could not parse suggested action: '{action_part}'")

    def step(self):
        """Perform one cycle of the Director's operation."""
        print("\n--- Director Phase ---")
        observation = self.observe()
        suggestion = self.think(observation)
        self.act(suggestion)
        print("--- End Director Phase ---")

============================== END FILE: src_GM\director.py ==============================


============================== START FILE: src_GM\interpreter.py ==============================

# interpreter.py
import re
import google.generativeai as genai
import json  # For parsing LLM output
import config


class LLMInterpreter:  
     def __init__(self, model, world_state):
          self.llm = model
          self.world = world_state  # Need world to get rules/state for prompt

     def interpret_and_resolve_action(self, agent_name, agent_location, utterance):
          """
               Uses LLM to understand intent, check feasibility, determine outcome,
               and return structured results including the event description.
               """
          print(
               f"[LLM Interpreter @ {agent_location}]: Resolving action for {agent_name}: '{utterance}'")

          # 1. Gather Context for Prompt
          world_rules, current_state_summary = self._get_relevant_world_context(
               agent_location)

          # 2. Craft the Prompt
          prompt = self._build_resolver_prompt(
               agent_name, agent_location, utterance, world_rules, current_state_summary
          )

          # 3. Call LLM
          try:
               # Configure LLM to ideally return JSON
               # Add JSON mode if available/needed
               response = self.llm.generate_content(prompt)
               raw_output = response.text

               # 4. Parse LLM Output
               result = self._parse_llm_output(raw_output)

               # 5. Log the Outcome Event (This is what others perceive)
               if result and result.get("outcome_description"):
                    # Log the LLM-generated outcome
                    self.world.log_event(
                    result["outcome_description"],
                    scope='action_outcome',  # New scope? Or keep 'action'?
                    location=agent_location,  # Or potentially new location if move succeeded
                    triggered_by=agent_name
                    )
               else:
                    # Log a failure to interpret if parsing failed
                    self.world.log_event(
                         f"{agent_name} does something unclear ('{utterance}').",
                    scope='action_outcome',
                    location=agent_location,
                    triggered_by=agent_name
                    )
                    print(
                         f"[LLM Interpreter Error]: Failed to parse LLM output: {raw_output}")
                    return None  # Indicate failure

               # 6. Apply Direct World State Changes (if specified by LLM)
               self._apply_state_updates(
                    agent_name, result.get("world_state_update"))

               return result  # Return the full structured result for logging/debugging

          except Exception as e:
               print(f"[LLM Interpreter Error]: LLM call or parsing failed: {e}")
               # Log generic failure event
               self.world.log_event(
                    f"{agent_name}'s action ('{utterance}') seems to fail or cause confusion.",
                    scope='action_outcome',
                    location=agent_location,
                    triggered_by=agent_name
               )
               return None


     def _get_relevant_world_context(self, location):
          # Fetch rules, connectivity, properties relevant to the location
          # Example:
          rules = f"Rules: Shelter door is currently {'locked' if self.world.location_properties.get('Shelter', {}).get('door_locked') else 'unlocked'}."
          connectivity = f"From {location}, you can reach: {self.world.location_connectivity.get(location, [])}."
          # Simplified
          state_summary = f"Others present: {self.world.get_agents_at(location)}."
          # Combine relevant parts
          return rules, f"{connectivity}\n{state_summary}"

     def _build_resolver_prompt(self, agent_name, location, utterance, rules, state_summary):
          # Construct the detailed prompt asking for JSON output
          # (This needs careful design based on desired capabilities)
          prompt = f"""You are the Action Resolver for a simulation.
     Agent '{agent_name}' is at location '{location}'.
     Their intended action is: "{utterance}"

     Relevant world state and rules:
     {rules}
     {state_summary}

     Analyze the agent's intended action based on the world state. Determine if it's possible.
     Output a JSON object describing the outcome:
     {{
     "action_type": "MOVE | SPEAK | INTERACT | OBSERVE | WAIT | FAIL",
     "parameters": {{ // action-specific details, e.g., "destination": "X", "target": "Y" }},
     "success": true | false,
     "reasoning": "Brief explanation of why it succeeded or failed.", // For debugging
     "outcome_description": "A short sentence describing what actually happened as an observer would see it.",
     "world_state_update": {{ // OPTIONAL: Direct changes needed, e.g., "agent_location": "NewLoc" }}
     }}

     Example Outcome (Move success): {{"action_type": "MOVE", "parameters": {{"destination": "Park"}}, "success": true, "reasoning": "Path to Park is clear.", "outcome_description": "{agent_name} walks towards the Park.", "world_state_update": {{"agent_location": "Park"}} }}
     Example Outcome (Move fail): {{"action_type": "MOVE", "parameters": {{"destination": "Shelter"}}, "success": false, "reasoning": "Shelter door is locked.", "outcome_description": "{agent_name} tries the Shelter door, but it's locked.", "world_state_update": null }}
     Example Outcome (Speak): {{"action_type": "SPEAK", "parameters": {{"target": "Bob", "message": "Hello"}}, "success": true, "reasoning": "Bob is present.", "outcome_description": "{agent_name} says to Bob, 'Hello'.", "world_state_update": null }}

     Your JSON Output:
     ```json
     """ # Expect JSON within ```json ... ``` block
          # Note: Using ```json block helps LLM format correctly
          return prompt

     def _parse_llm_output(self, raw_output):
          try:
               # Extract JSON from potential markdown code blocks
               json_match = re.search(r'```json\s*([\s\S]+?)\s*```', raw_output)
               if json_match:
                    json_str = json_match.group(1)
               else:
                    # Assume raw output might be JSON directly if no block found
                    json_str = raw_output

               data = json.loads(json_str)
               # Basic validation (add more checks as needed)
               if 'action_type' in data and 'success' in data and 'outcome_description' in data:
                    return data
               else:
                    print("[LLM Interpreter Warning]: Parsed JSON missing required keys.")
                    return None
          except json.JSONDecodeError as e:
               print(
                    f"[LLM Interpreter Error]: JSON Decode failed: {e}. Raw output: {raw_output}")
               return None
          except Exception as e:
               print(f"[LLM Interpreter Error]: Unexpected parsing error: {e}")
               return None


     def _apply_state_updates(self, agent_name, updates):
          if not updates:
               return
          print(
               f"[LLM Interpreter]: Applying state updates for {agent_name}: {updates}")
          if "agent_location" in updates:
               new_loc = updates["agent_location"]
               # Directly update, assuming LLM validated reachability
               if new_loc in self.world.location_descriptions:
                    self.world.agent_locations[agent_name] = new_loc
                    # Note: We might need adjustment if move_agent had other side effects
               else:
                    print(
                         f"[LLM Interpreter Error]: LLM suggested moving {agent_name} to invalid location '{new_loc}'!")
          # Add handlers for other potential updates (e.g., inventory changes)


============================== END FILE: src_GM\interpreter.py ==============================


============================== START FILE: src_GM\main.py ==============================

# main.py
import time
import google.generativeai as genai
import importlib

# Import our modules
import config
from world import WorldState
from agent.agent import Agent
from director import Director # Import the new Director class
# Import base and initial implementation
from action_resolver import BaseActionResolver, LLMActionResolver

# --- Factory Functions (or simple conditionals) ---


def get_memory_module(memory_type):
    if memory_type == "SimpleMemory":
        from agent.memory import SimpleMemory
        return SimpleMemory()
    # Elif memory_type == "VectorMemory":
    #    from agent.memory import VectorMemory
    #    return VectorMemory(...) # Needs setup
    else:
        raise ValueError(f"Unknown memory type: {memory_type}")


def get_thinking_module(thinker_type, model):
    if thinker_type == "GeminiThinker":
        from agent.thinking import GeminiThinker
        return GeminiThinker(model)
    # Elif thinker_type == "RuleBasedThinker":
    #    from agent.thinking import RuleBasedThinker
    #    return RuleBasedThinker(...)
    else:
        raise ValueError(f"Unknown thinker type: {thinker_type}")


def get_action_resolver(resolver_type, model, world_ref=None):  # Pass model if needed
    if resolver_type == "LLMResolver":
        # LLMActionResolver might need the model
        return LLMActionResolver(model, world_ref)
    # Elif resolver_type == "StructuredValidator":
    #    from action_resolver import StructuredValidatorResolver
    #    return StructuredValidatorResolver() # Might not need model
    # Elif resolver_type == "Passthrough":
    #     from action_resolver import PassthroughResolver
    #     return PassthroughResolver()
    else:
        raise ValueError(f"Unknown action resolver type: {resolver_type}")
    
def run_simulation():
    print("--- Starting Modular Free Agent Simulation with Director ---")
    print(f"Config: Memory={config.AGENT_MEMORY_TYPE}, Thinker={config.AGENT_THINKER_TYPE}, Resolver={config.ACTION_RESOLVER_TYPE}, Perception={config.EVENT_PERCEPTION_MODEL}")
    
    # 1. Initialize LLM Model
    print(f"Configuring Gemini model: {config.MODEL_NAME}")
    genai.configure(api_key=config.GEMINI_API_KEY)
    model = genai.GenerativeModel(
        model_name=config.MODEL_NAME,
        generation_config=config.GENERATION_CONFIG,
        safety_settings=config.SAFETY_SETTINGS
    )
    print("Model configured.")

    # 2. Initialize World State
    world = WorldState(locations=config.KNOWN_LOCATIONS)
    world.global_context['weather'] = "Clear" # Initial weather

    # 3. Initialize Agents
    agents = []
    agent_configs = [
        {"name": "Alice", "personality": config.DEFAULT_PERSONALITIES.get("Alice", "default")},
        {"name": "Bob", "personality": config.DEFAULT_PERSONALITIES.get("Bob", "default")}
    ]
    for agent_conf in agent_configs:
        agent_name = agent_conf["name"]
        memory = get_memory_module(config.AGENT_MEMORY_TYPE)
        thinker = get_thinking_module(config.AGENT_THINKER_TYPE, model)
        agent = Agent(
            name=agent_name,
            personality=agent_conf["personality"],
            memory_module=memory,
            thinking_module=thinker
        )
        agents.append(agent)
        start_location = "Park"
        world.add_agent_to_location(
            agent_name, start_location, triggered_by="Setup")
        world.register_agent(agent)

    # 4. Initialize Director (remains mostly the same, observes world state)
    director_model = model  # Director might need its own model instance later?
    director = Director(world, director_model, config.NARRATIVE_GOAL)

    ## 5. Initialize Action Resolver
    action_resolver = get_action_resolver(
        config.ACTION_RESOLVER_TYPE, model, world_ref=world)
    
    # --- Simulation Steps ---
    step = 0
    while step < config.SIMULATION_MAX_STEPS:
        step += 1
        world.advance_step() # Increment world step counter
        print(f"\n{'='*15} Simulation Step {step}/{config.SIMULATION_MAX_STEPS} {'='*15}")
        print(world.get_full_state_string())

        # --- Agent Thinking Phase ---
        # Agents decide their *intended* actions based on perceived events & memory
        agent_intentions = {}  # agent_name -> intended_action_output
        agent_current_locations = {}  # Store locations for resolver
        for agent in agents:
            print(f"\n--- Processing {agent.name} ---")
            # Get agent's location BEFORE they act (in case they move)
            current_loc = world.agent_locations.get(agent.name, None)
            if not current_loc:
                print(f"[Sim Warning]: Agent {agent.name} has no location! Skipping.")
                continue
            agent_current_locations[agent.name] = current_loc

            # Agent perceives dispatched events (handled by world.log_event -> agent.perceive)
            # Agent thinks based on memory (inc. perceptions) and static context
            # Agent updates own memory with intent
            intended_output = agent.think(world)
            agent_intentions[agent.name] = intended_output
            time.sleep(1.0)  # LLM rate limiting/pause

        # --- Action Resolution Phase ---
        # The Action Resolver interprets intentions and determines outcomes
        print("\n--- Action Resolution Phase ---")
        resolution_results = {}  # agent_name -> resolution_dict
        all_state_updates = []  # Collect updates from all agents first
        all_outcome_events = []  # Collect outcome events
        
        for agent_name, intent in agent_intentions.items():
            agent_loc = agent_current_locations.get(agent_name)
            if agent_loc and action_resolver:
                print(f"-- Resolving for {agent_name} at {agent_loc} --")
                result = action_resolver.resolve(
                    agent_name, agent_loc, intent, world
                )
                resolution_results[agent_name] = result
                if result and result.get("success"):
                    print(
                        f"[Resolver OK] {agent_name}: {result.get('action_type')} -> {result.get('outcome_description')}")
                    if result.get("world_state_updates"):
                        all_state_updates.extend(result["world_state_updates"])
                    # Create the event tuple for logging *after* state updates
                    all_outcome_events.append(
                        (result['outcome_description'],
                        'action_outcome', agent_loc, agent_name)
                    )
                elif result:  # Handled failure case
                    print(
                        f"[Resolver FAIL] {agent_name}: {result.get('reasoning', 'Unknown reason')}")
                    # Log the failure outcome event
                    all_outcome_events.append(
                        (result['outcome_description'],
                        'action_outcome', agent_loc, agent_name)
                    )
                else:  # Severe failure in resolver itself
                    print(
                        f"[Resolver ERROR] Critical failure resolving for {agent_name}")
                    # Log a generic system failure event?
                    all_outcome_events.append(
                        (f"System error resolving {agent_name}'s action.",
                        'system_error', agent_loc, 'System')
                    )
            else:
                print(
                    f"[Sim Warning]: Cannot resolve action for {agent_name}, location or resolver unknown.")
            time.sleep(0.5)

        # --- Director Phase --- 
        director.step() # Let the director observe, think, and act
        time.sleep(1.0) # Optional pause after director

        # --- World Update Phase ---
        # Apply all accumulated state changes atomically (conceptually)
        print("\n--- World Update Phase ---")
        if all_state_updates:
            world.apply_state_updates(
                all_state_updates, triggered_by="AgentActions")
        else:
            print("No world state updates required.")

        # Log all outcome events AFTER state updates are done
        print("\n--- Logging Action Outcomes ---")
        for desc, scope, loc, trig_by in all_outcome_events:
            # This will dispatch perceptions
            world.log_event(desc, scope, loc, trig_by)
            
        # --- Manual Control / End Step ---
        
        print("\n--- End of Step ---")
        print(world.get_full_state_string()) # Show final state for the step

        # Manual stepping and limited override
        user_input = input("Enter for next step, 'goal <new goal>' to change director goal, 'q' to quit: ").lower().strip()

        if user_input == 'q':
            print("Quitting simulation by user request.")
            break
        elif user_input.startswith('goal '):
            new_goal = user_input[len('goal '):].strip()
            if new_goal:
                print(f"Updating Director goal to: '{new_goal}'")
                director.narrative_goal = new_goal
                world.log_event(f"COMMAND: Director narrative goal updated to '{new_goal}'.", triggered_by="User")
            else:
                print("Invalid command. Use 'goal <description>'.")
        elif user_input.startswith('w '): # Keep manual weather override? Optional.
             new_weather = user_input[len('w '):].strip().title()
             if new_weather:
                 print(f"Manual weather override to: {new_weather}")
                 world.set_weather(new_weather, triggered_by="User")
             else:
                 print("Invalid command. Use 'w <condition>'.")
        elif user_input:
            print(f"Unknown command: '{user_input}'")
        # Pressing Enter continues

    print(f"\n--- Simulation Ended after {step} steps ---")

# --- Run ---
if __name__ == "__main__":
    run_simulation()


============================== END FILE: src_GM\main.py ==============================


============================== START FILE: src_GM\world.py ==============================

# src/world.py
import config
from collections import namedtuple

# Define a structure for events for clarity
Event = namedtuple(
    "Event", ["description", "location", "scope", "step", "triggered_by"])


class WorldState:
    def __init__(self, locations):
        self.agent_locations = {}  # agent_name -> location_name
        self.location_descriptions = locations
        self.location_connectivity = {  # Defines possible direct movements
            "Park": ["Shelter", "Forest Edge"],
            "Shelter": ["Park"],
            "Forest Edge": ["Park"]
            # Add more connections as locations are added
        }
        self.location_properties = {  # Defines states/features of locations
            "Park": {"ground": "grassy"},
            # Example: Shelter door starts unlocked
            "Shelter": {"door_locked": False, "contains": []},
            "Forest Edge": {"terrain": "uneven"}
        }
        self.global_context = {"weather": "Clear"}
        self.event_log = []  # Now stores Event tuples
        self.current_step = 0  # Track simulation step

        # agent_name -> agent_object (for dispatch)
        self.registered_agents = {}
        
        print("WorldState initialized.")

    def register_agent(self, agent):
        """Registers an agent to receive events."""
        if agent.name not in self.registered_agents:
            self.registered_agents[agent.name] = agent
            print(
                f"[World Event Dispatch]: Registered {agent.name} for events.")
    
    def unregister_agent(self, agent_name):
        """Unregisters an agent."""
        if agent_name in self.registered_agents:
            del self.registered_agents[agent_name]
            print(f"[World Event Dispatch]: Unregistered {agent_name}.")

    def advance_step(self):
        self.current_step += 1

    # --- NEW: Helper methods to access rules ---
    def get_reachable_locations(self, from_location):
        """Returns list of locations directly reachable from the given one."""
        return self.location_connectivity.get(from_location, [])

    def get_location_property(self, location, prop_name):
        """Safely gets a property of a location."""
        return self.location_properties.get(location, {}).get(prop_name, None)

    def set_location_property(self, location, prop_name, value, triggered_by="System"):
        """Sets a property, potentially logging an event via log_event."""
        if location not in self.location_properties:
            print(
                f"[World State Warning]: Location '{location}' not found for setting property.")
            return False
        if prop_name not in self.location_properties[location]:
            print(
                f"[World State Warning]: Property '{prop_name}' doesn't exist for '{location}'. Adding it.")
            # Decide if dynamic property creation is allowed or should error

        old_value = self.location_properties[location].get(prop_name, 'None')
        if old_value != value:
            self.location_properties[location][prop_name] = value
            # Log the change as an event? Depends on granularity needed.
            # Example: log the *effect* rather than the state change itself.
            # e.g., if setting door_locked=False, the event might be "The shelter door unlocks"
            print(
                f"[World State Update]: Property '{prop_name}' of '{location}' changed to '{value}' (Trigger: {triggered_by}).")
            return True
        return False
    
    # --- End NEW Helpers ---

    def add_agent_to_location(self, agent_name, location_name, triggered_by="Setup"):
        """Adds agent and updates location state if needed."""
        if location_name in self.location_descriptions:
            old_location = self.agent_locations.get(agent_name)
            self.agent_locations[agent_name] = location_name
            print(f"World: Agent {agent_name} now at {location_name}")

            # Update 'contains' property if location has it
            if old_location and old_location != location_name:
                if 'contains' in self.location_properties.get(old_location, {}):
                    if agent_name in self.location_properties[old_location]['contains']:
                        self.location_properties[old_location]['contains'].remove(
                            agent_name)

            if 'contains' in self.location_properties.get(location_name, {}):
                if agent_name not in self.location_properties[location_name]['contains']:
                    self.location_properties[location_name]['contains'].append(
                        agent_name)

            # Log the arrival event (could be refined based on triggered_by)
            if triggered_by != "Setup":  # Don't log redundant 'appears' if moving
                self.log_event(f"{agent_name} arrives.", scope='local',
                                location=location_name, triggered_by=triggered_by)
            elif not old_location:  # Log initial appearance
                self.log_event(f"{agent_name} appears in the {location_name}.",
                                scope='local', location=location_name, triggered_by=triggered_by)
        else:
            print(
                f"Warning: Cannot move {agent_name} to unknown location '{location_name}'")

    def get_agents_at(self, location_name):
        # Add simple check if location exists
        if location_name not in self.location_descriptions:
            return []
        return [name for name, loc in self.agent_locations.items() if loc == location_name]

    def log_event(self, description, scope='local', location=None, triggered_by="Simulation", dispatch=True):
        """Logs an event and dispatches it to relevant registered agents."""
        new_event = Event(description=description, location=location,
                          scope=scope, step=self.current_step, triggered_by=triggered_by)
        self.event_log.append(new_event)
        log_prefix = f"[Event Log Step {self.current_step}][{triggered_by} @ {location or 'Global'}/{scope}]"
        print(f"{log_prefix}: {description}")

        # Trim log if needed
        if len(self.event_log) > config.MAX_RECENT_EVENTS * 2:  # Keep a longer internal log
            self.event_log.pop(0)

        # --- Event Dispatch Logic ---
        if dispatch and config.EVENT_PERCEPTION_MODEL == "DirectDispatch":
            dispatched_to = []
            for agent_name, agent_obj in self.registered_agents.items():
                agent_current_loc = self.agent_locations.get(agent_name)
                should_perceive = False
                # Global events are perceived by everyone
                if scope == 'global':
                    should_perceive = True
                # Local events are perceived by agents at that location
                elif scope == 'local' and location == agent_current_loc:
                    should_perceive = True
                # Action outcomes might be perceived by others at the location
                elif scope == 'action_outcome' and location == agent_current_loc:
                    # Avoid agent perceiving echo of their own action description? Maybe filter here.
                    # if triggered_by != agent_name:  # Simple filter: don't dispatch action outcome to self
                    #     should_perceive = True
                    # Or let agent memory handle duplicates? Simpler for now.
                    should_perceive = True

                if should_perceive:
                    try:
                        agent_obj.perceive(new_event)
                        dispatched_to.append(agent_name)
                    except Exception as e:
                        print(
                            f"[Dispatch Error]: Failed to dispatch event to {agent_name}: {e}")
            if dispatched_to:
                print(
                    f"[World Event Dispatch]: Event dispatched to: {dispatched_to}")


    def set_weather(self, new_weather, triggered_by="Simulation"):
        """Changes the weather and logs the event."""
        old_weather = self.global_context.get('weather', 'unknown')
        if old_weather != new_weather:
            self.global_context['weather'] = new_weather
            # Log weather change as a GLOBAL event
            self.log_event(f"The weather changes from {old_weather} to {new_weather}.",
                           scope='global',
                           location=None,  # Global event has no specific location
                           triggered_by=triggered_by)
            return True
        return False

    def get_static_context_for_agent(self, agent_name):
         """Provides minimal, relatively static context."""
         location = self.agent_locations.get(agent_name)
         if not location: return "You are lost."

         context = f"Current Location: {location} ({self.location_descriptions.get(location, 'Unknown')}).\n"
         context += f"Current Weather: {self.global_context.get('weather', 'Unknown')}.\n"
         exits = self.get_reachable_locations(location)
         context += f"Visible Exits: {exits if exits else 'None'}.\n"
         # Add other static elements if needed (e.g., visible objects)
         return context

    def get_full_state_string(self):
        """For debugging - shows more structured log."""
        state = f"--- World State (Step: {self.current_step}) ---\n"
        state += f"Global Context: {self.global_context}\n"
        state += f"Agent Locations: {self.agent_locations}\n"
        state += f"Location Properties: {self.location_properties}\n"
        # Show who listens
        state += f"Registered Agents: {list(self.registered_agents.keys())}\n"
        state += f"Event Log ({len(self.event_log)} total, showing last {config.MAX_RECENT_EVENTS}):\n"
        display_events = self.event_log[-config.MAX_RECENT_EVENTS:]
        for event in display_events:
            state += f"  - St{event.step} [{event.triggered_by}@{event.location or 'Global'}/{event.scope}] {event.description}\n"
        return state + "-------------------"

    def apply_state_updates(self, updates: list, triggered_by: str):
        """Applies a list of state changes suggested by the Action Resolver."""
        if not updates:
            return

        print(
            f"[World State Apply]: Applying {len(updates)} updates triggered by {triggered_by}.")
        for update in updates:
            try:
                update_type = update[0]
                target = update[1]
                value = update[2]

                if update_type == "agent_location":
                    # Value is the new location name
                    self.add_agent_to_location(
                        agent_name=target, location_name=value, triggered_by=triggered_by)
                elif update_type == "location_property":
                    # Target is location name, value is prop_name, need 4th element for prop_value
                    if len(update) == 4:
                        prop_name = value
                        prop_value = update[3]
                        self.set_location_property(
                            location=target, prop_name=prop_name, value=prop_value, triggered_by=triggered_by)
                    else:
                        print(
                            f"[World State Apply Error]: Invalid format for location_property update: {update}")
                # Add more update types here (e.g., global context, agent inventory)
                else:
                    print(
                        f"[World State Apply Warning]: Unknown update type '{update_type}'")

            except IndexError as e:
                print(
                    f"[World State Apply Error]: Malformed update tuple {update}: {e}")
            except Exception as e:
                print(
                    f"[World State Apply Error]: Failed to apply update {update}: {e}")


============================== END FILE: src_GM\world.py ==============================


============================== START FILE: src_GM\agent\agent.py ==============================

# agent.py
# No longer needs direct LLM imports, relies on Thinker/Memory modules

class Agent:
    def __init__(self, name, personality, memory_module, thinking_module):
        self.name = name
        self.personality = personality
        self.memory = memory_module # An instance of BaseMemory
        self.thinker = thinking_module # An instance of BaseThinker
        self.action_buffer = None  # Store the output of think() before resolution
        print(f"Agent {name} initialized with {type(memory_module).__name__} and {type(thinking_module).__name__}.")

    def perceive(self, event):
        """Processes a perceived event from the world and stores it in memory."""
        # Simple formatting for now, could be more sophisticated
        # Avoid adding perception of own action's *outcome* if resolver logs it better?
        # Let's keep it simple: add all perceived events. Memory can handle redundancy.
        perception_text = f"[Perception @ Step {event.step}] ({event.scope} at {event.location or 'Global'} by {event.triggered_by}): {event.description}"
        self.memory.add_observation(perception_text)
        print(f"DEBUG {self.name} Perceived: {perception_text}") # Optional debug

    def think(self, world_state):
        """
        Agent's thinking cycle. Uses memory and thinker to decide next action intent.
        Does NOT execute the action, just returns the intended output.
        """
        # 1. Get memory context
        memory_context = self.memory.get_memory_context()

        # 2. Get minimal static world context (if needed by the thinker)
        # Pass only what's necessary, avoid full dynamic state if using event perception
        static_context = world_state.get_static_context_for_agent(self.name)

        # 3. Think (call the thinking module)
        # Pass agent reference (for personality), static context, and memory context
        action_output = self.thinker.generate_output(
            self, static_context, memory_context)

        # 4. Store intended action (important!)
        self.action_buffer = action_output
        # Also add own *intended* action to memory for self-reflection
        self.memory.add_observation(
            f"[My Intent @ Step {world_state.current_step}] {action_output}")

        return action_output

    # Note: Memory update for the *result* of the action now happens implicitly
    # when the ActionResolver's outcome event is logged and dispatched back
    # to the agent via perceive().


============================== END FILE: src_GM\agent\agent.py ==============================


============================== START FILE: src_GM\agent\memory.py ==============================

# memory.py
import config
from abc import ABC, abstractmethod # For defining abstract base class

class BaseMemory(ABC):
    """Abstract base class for agent memory modules."""
    @abstractmethod
    def add_observation(self, observation_text):
        """Adds a piece of information (perception, action result, thought) to memory."""
        pass

    @abstractmethod
    def get_memory_context(self):
        """Returns a string summary of relevant memories for the LLM prompt."""
        pass

    @abstractmethod
    def clear(self):
        """Clears the memory."""
        pass

class SimpleMemory(BaseMemory):
    """A basic rolling string buffer memory."""
    def __init__(self, max_length=config.MAX_MEMORY_TOKENS):
        self.memory_buffer = ""
        self.max_length = max_length # Approximate character length

    def add_observation(self, observation_text:str):
        # Add new observation, ensuring separation
        new_entry = observation_text.strip()
        if self.memory_buffer:
            self.memory_buffer = f"{self.memory_buffer}\n{new_entry}"
        else:
            self.memory_buffer = new_entry

        # Trim if exceeds max length (simple truncation from the beginning)
        if len(self.memory_buffer) > self.max_length:
            excess = len(self.memory_buffer) - self.max_length
            # Try to cut off at a newline to keep entries somewhat intact
            first_newline = self.memory_buffer.find('\n', excess)
            if first_newline != -1:
                 self.memory_buffer = self.memory_buffer[first_newline+1:]
            else: # If no newline found after excess, just truncate
                 self.memory_buffer = self.memory_buffer[excess:]
        print(f"DEBUG Memory Add: Added '{new_entry[:50]}...'. Buffer size: {len(self.memory_buffer)}") # Debug

    def get_memory_context(self):
        if not self.memory_buffer:
            return "No specific memories recalled."
        # Provide the most recent memories
        return f"Recollections (most recent last):\n{self.memory_buffer}"

    def clear(self):
        self.memory_buffer = ""

# Example of how you might add another memory type later:
# class VectorMemory(BaseMemory):
#     def __init__(self, embedding_model, vector_db):
#         # ... implementation using embeddings ...
#         pass
#     def add_observation(self, observation_text):
#         # ... embed and store ...
#         pass
#     def get_memory_context(self, query_text):
#         # ... retrieve relevant memories based on query ...
#         pass
#     def clear(self):
#         # ... clear db ...
#         pass

============================== END FILE: src_GM\agent\memory.py ==============================


============================== START FILE: src_GM\agent\thinking.py ==============================

# thinking.py
import google.generativeai as genai
import config
from abc import ABC, abstractmethod

class BaseThinker(ABC):
    """Abstract base class for agent thinking/decision-making modules."""
    @abstractmethod
    def generate_output(self, agent, static_world_context, memory_context):  # Renamed
        """Generates the agent's next intended action/thought/speech output."""
        pass

class GeminiThinker(BaseThinker):
    """Uses Google Gemini LLM to generate agent utterances."""
    def __init__(self, model):
        self.llm = model # Pass the initialized model instance

    def generate_output(self, agent, static_world_context, memory_context):  # Renamed
        """Formats prompt and calls the Gemini API."""

        prompt = f"""You are {agent.name}, a character in a simulated world.
Your personality: {agent.personality}.

Your current static situation:
{static_world_context}

Your recent memories and perceptions (most recent last):
{memory_context}

Based *only* on the above, what do you intend to think, say, or do next?
Describe your intended action, thought, or utterance in a single, short, natural sentence.
Focus on your intent. Be concise.
Examples:
- Try to walk towards the Forest Edge.
- Ask Bob, "Did you hear that noise?"
- Examine the ground near the shelter.
- Think about how cold the weather is getting.
- Decide to wait and see what happens.

Your intended output (one sentence):"""  # Changed 'response' to 'intended output'

        print(f"\n[{agent.name} is thinking...]")
        # print(f"--- DEBUG PROMPT for {agent.name} ---\n{prompt}\n--------------------")

        try:
            response = self.llm.generate_content(prompt)
            utterance = response.text.strip().split('\n')[0]

            if not utterance or len(utterance) < 5:
                print(
                    f"[{agent.name} Warning]: LLM gave short/empty response: '{utterance}'. Defaulting to wait intent.")
                utterance = f"Intend to wait silently."
            else:
                print(f"[{agent.name} intends]: {utterance}")

            return utterance

        except Exception as e:
            print(f"[{agent.name} Error]: LLM generation failed: {e}")
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                print(
                    f"[{agent.name} Safety Block]: Reason: {response.prompt_feedback.block_reason}")
            return f"Intend to pause due to confusion."  # Return an intent


============================== END FILE: src_GM\agent\thinking.py ==============================
