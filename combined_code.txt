
============================== START FILE: src_GM\action_resolver.py ==============================
 ```
python

import google.generativeai as genai  # Keep LLM import here if resolver uses it
import json
import re
from abc import ABC, abstractmethod
from world import WorldState
import config

class BaseActionResolver(ABC):
    """
    Abstract Base Class for interpreting an agent's intended action output
    and determining its outcome within the world state.
    Different implementations will represent different validation/resolution strategies.
    """
    @abstractmethod
    def resolve(self, agent_name: str, agent_location: str, action_output: str, world_state) -> dict:
        """
        Takes the agent's raw output and the current world state,
        determines the actual outcome, and returns a structured result.

        Args:
            agent_name: Name of the agent attempting the action.
            agent_location: Location of the agent when attempting the action.
            action_output: The raw output from the agent's thinking module.
            world_state: Reference to the current WorldState.

        Returns:
            A dictionary containing resolution details, e.g.:
            {
                "success": bool,
                "action_type": "MOVE" | "SPEAK" | "INTERACT" | "OBSERVE" | "WAIT" | "FAIL" | "UNKNOWN",
                "parameters": dict, # Action specific details derived by the resolver
                "outcome_description": str, # What an observer sees happen
                # List of direct state changes, e.g., [('agent_location', agent_name, 'NewLoc'), ('lock_state', 'ShelterDoor', False)]
                "world_state_updates": list,
            }
            Return None or a dict with success=False if resolution fails severely.
        """
        pass

# --- Concrete Implementation (Moves logic from LLMInterpreter) ---

class LLMActionResolver(BaseActionResolver):
    """
    Uses an LLM to interpret natural language action output, validate it
    against world rules (potentially simplified), and generate an outcome.
    (Based on the original LLMInterpreter logic)
    """

    def __init__(self, model, world_state_ref_for_prompting_rules=None):
        self.llm = model
        # Might need a way to get *some* world info for the prompt,
        # but avoid passing the full mutable state if possible.
        # Maybe pass specific rule functions or data? For now, keep it simple.
        self.world_ref = world_state_ref_for_prompting_rules  # Use carefully

    def resolve(self, agent_name: str, agent_location: str, action_output: str, world_state:WorldState) -> dict:
        if config.SIMULATION_MODE == 'debug' or config.SIMULATION_MODE == 'only_resolver':
            print(
                f"[LLM Resolver @ {agent_location}]: Resolving for {agent_name}: '{action_output}'")

        # 1. Gather Context (Simplified example - adapt as needed)
        # This part needs careful design - what *minimal* context does the resolver need?
        # Avoid giving it the full dynamic event history if possible.
        # rules = f"Rules: Shelter door currently {world_state.get_location_property('Shelter', 'door_locked')}."
        connectivity = f"From {agent_location}, exits lead to: {world_state.get_reachable_locations(agent_location)}."
        agents_present = world_state.get_agents_at(agent_location)
        others_present = [
            name for name in agents_present if name != agent_name]
        state_summary = f"Others present: {others_present if others_present else 'None'}."

        # 2. Craft Prompt (Similar to old Interpreter prompt, but focused on resolution)
        prompt = f"""You are the Action Resolver for a simulation.
Agent '{agent_name}' at location '{agent_location}' intends to: "{action_output}"

Relevant world state and rules:
{connectivity}
{state_summary}
Weather: {world_state.global_context.get('weather', 'Clear')}

Analyze the agent's intent. Is it possible? What is the most plausible outcome?
Output a JSON object describing the outcome:
{{
    "success": true | false,
    "action_type": "MOVE | SPEAK | INTERACT | OBSERVE | WAIT | FAIL | UNKNOWN",
    "parameters": {{ // e.g., "destination": "X", "target": "Y", "message": "..." }},
    "outcome_description": "Short sentence of what an observer sees.",
    "world_state_updates": [ // OPTIONAL: List of ['attribute', 'target', 'new_value'] tuples
        // e.g., ["agent_location", "{agent_name}", "Park"], ["location_property", "Shelter", "door_locked", false]
    ]
}}

Example (Move success): {{"success": true, "action_type": "MOVE", "parameters": {{"destination": "Park"}}, "outcome_description": "{agent_name} walks towards the Park.", "world_state_updates": [["agent_location", "{agent_name}", "Park"]] }}
Example (Move fail): {{"success": false, "action_type": "MOVE", "parameters": {{"destination": "Shelter"}}, "outcome_description": "{agent_name} tries the Shelter door, but it's locked.", "world_state_updates": [] }}
Example (Speak): {{"success": true, "action_type": "SPEAK", "parameters": {{"target": "Bob", "message": "Hello"}}, "outcome_description": "{agent_name} says to Bob, 'Hello'.", "world_state_updates": [] }}

Your JSON Output:
```json
"""

    # 3. Call LLM & Parse (Similar parsing logic as before)
        try:
            # Add JSON mode if available/needed
            response = self.llm.generate_content(
                prompt)  # Assuming self.llm is configured
            raw_output = response.text
            # Parse JSON (reuse robust parsing logic from your interpreter)
            parsed_json = self._parse_llm_output(raw_output)

            if parsed_json and 'success' in parsed_json and 'outcome_description' in parsed_json:
                # Basic validation passed
                # Ensure 'world_state_updates' is a list, default to empty if missing
                parsed_json['world_state_updates'] = parsed_json.get(
                    'world_state_updates', [])
                return parsed_json
            else:
                print(
                    f"[LLM Resolver Error]: Failed to parse or validate LLM output: {raw_output}")
                    # Return a generic failure dictionary
                return {
                        "success": False,
                    "action_type": "FAIL",
                    "parameters": {"raw_output": action_output},
                    "outcome_description": f"{agent_name} does something unclear or fails ('{action_output}').",
                    "world_state_updates": []
                    }

        except Exception as e:
            print(f"[LLM Resolver Error]: LLM call or processing failed: {e}")
            return {  # Generic failure dictionary
                "success": False, "action_type": "FAIL", "parameters": {"raw_output": action_output},
                "outcome_description": f"{agent_name}'s action ('{action_output}') causes confusion or fails.",
                "world_state_updates": []
            }

    def _parse_llm_output(self, raw_output):
        json_str = raw_output # Start with the raw output
        try:
            # Attempt to extract JSON from markdown code block first
            json_match = re.search(r'```json\s*([\s\S]+?)\s*```', raw_output, re.IGNORECASE)
            if json_match:
                json_str = json_match.group(1).strip()
            else:
                # If no markdown block, assume the whole output might be JSON (or needs fixing)
                # Find the first '{' and last '}' to potentially isolate JSON-like content
                start_brace = raw_output.find('{')
                end_brace = raw_output.rfind('}')
                if start_brace != -1 and end_brace != -1 and end_brace > start_brace:
                    json_str = raw_output[start_brace:end_brace+1].strip()
                # else: keep raw_output as json_str if no braces found

            # First attempt to parse the extracted/raw string
            data = json.loads(json_str)
            
            # Ensure all required fields are present
            if 'success' in data and 'action_type' in data:
                # Add default outcome_description if missing
                if 'outcome_description' not in data:
                    agent_name = "the agent"  # Default fallback
                    action_type = data.get('action_type', 'UNKNOWN')
                    if action_type == "SPEAK" and 'parameters' in data and 'message' in data['parameters']:
                        data['outcome_description'] = f"{agent_name} says, '{data['parameters']['message']}'"
                    else:
                        data['outcome_description'] = f"{agent_name} performs a {action_type.lower()} action."
                
                # Ensure world_state_updates is a list
                if 'world_state_updates' not in data:
                    data['world_state_updates'] = []
                    
            return data

        except json.JSONDecodeError as e:
            print(f"[LLM Resolver Warning]: Initial JSON Decode failed: {e}. Attempting LLM fix...")
            print(f"--- Faulty JSON String ---\n{json_str}\n------------------------")

            # --- LLM Fix Attempt using a specialized model ---
            try:
                # Configure and instantiate a dedicated model for JSON fixing
                # Lower temperature for more deterministic output
                fixer_generation_config = {
                    "temperature": 0.1, # Low temperature for focused fixing
                    "top_p": 0.95,
                    "top_k": 50,
                    "max_output_tokens": 1024, # Allow potentially larger fixed JSON
                }
                # Assuming config.MODEL_NAME and config.SAFETY_SETTINGS are accessible
                # You might need to import 'config' if not already done in this file
                fixer_model = genai.GenerativeModel(
                    model_name=config.MODEL_NAME, # Or a specific model if desired
                    generation_config=fixer_generation_config,
                )

                # Slightly relaxed prompt, still discouraging extra text
                fix_prompt = f"""The following text is supposed to be a valid JSON object, but it failed parsing due to syntax errors. Please correct the syntax errors and output ONLY the corrected JSON object. Aim to output *only* the raw, valid JSON object itself. Do not include any explanations or apologies.

Broken JSON string:
{json_str}

Corrected JSON object:""" # Removed the explicit "Do not include ... ```json ... ```"

                # Use the specialized fixer_model
                fix_response = fixer_model.generate_content(fix_prompt)
                fixed_json_raw = fix_response.text.strip()

                # Handle potential markdown block in the fixer's response
                final_json_str = fixed_json_raw
                fixed_json_match = re.search(r'```json\s*([\s\S]+?)\s*```', fixed_json_raw, re.IGNORECASE)
                if fixed_json_match:
                     final_json_str = fixed_json_match.group(1).strip()
                # else: use the stripped raw response if no markdown block found
                if config.SIMULATION_MODE == 'debug' or config.SIMULATION_MODE == 'only_resolver':
                    print(f"--- LLM Fixer Model Suggestion ---\n{final_json_str}\n--------------------------------")
                fixed_data = json.loads(final_json_str) # Parse the potentially extracted string
                print("[LLM Resolver Info]: Successfully parsed LLM-fixed JSON.")
                return fixed_data

            except json.JSONDecodeError as fix_e:
                print(f"[LLM Resolver Error]: LLM fix failed - JSON Decode failed again: {fix_e}. Fixed attempt: {final_json_str}")
                return None
            except Exception as fix_e:
                # Catch errors during fixer model instantiation or generation
                print(f"[LLM Resolver Error]: LLM fix attempt failed with exception: {fix_e}")
                return None
            # --- End LLM Fix Attempt ---

        except Exception as e:
            # Catch any other unexpected errors during initial processing
            print(f"[LLM Resolver Error]: Unexpected parsing error before fix attempt: {e}. Raw output: {raw_output}")
            return None


```
============================== END FILE: src_GM\action_resolver.py ==============================


============================== START FILE: src_GM\config.py ==============================
 ```
python

# config.py
import os
from dotenv import load_dotenv

# Load API Key
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError(
        "Gemini API Key not found. Make sure it's set in the .env file.")

# LLM Generation Settings
GENERATION_CONFIG = {
    "temperature": 1.3,
    "top_p": 0.95,
    "top_k": 50,
    "max_output_tokens": 200,  # Increased to allow for more detailed responses
}

# LLM Safety Settings
# SAFETY_SETTINGS = [
#     {"category": "HARM_CATEGORY_HARASSMENT",
#         "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
#     {"category": "HARM_CATEGORY_HATE_SPEECH",
#         "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
#     {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
#         "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
#     {"category": "HARM_CATEGORY_DANGEROUS_CONTENT",
#         "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
# ]


# Model Name
MODEL_NAME = "gemini-2.0-flash-lite"

# Simulation Settings
MAX_RECENT_EVENTS = 15
MAX_MEMORY_TOKENS = 600
SIMULATION_MAX_STEPS = 30

# World Settings
KNOWN_LOCATIONS = {
    "Park": "A wide open park area with some trees.",
    "Shelter": "A simple wooden shelter.",
    "Forest Edge": "The edge of a dark forest."
}

# Agent Settings
DEFAULT_PERSONALITIES = {
    "Alice": "curious, slightly anxious, observant",
    "Bob": "calm, pragmatic, speaks plainly"
}
# --- Component Selection ---
# Choose the implementations for different parts of the simulation
# (Allows easy switching for experiments)

# Options: "SimpleMemory", "VectorMemory" (future)
AGENT_MEMORY_TYPE = "ShortLongTMemory"
# Options: "GeminiThinker", "RuleBasedThinker" (future)
AGENT_PLANNING_TYPE = "GeminiThinker"

# Action Resolution Strategy (Crucial for your experiments!)
# Options: "LLMResolver", "StructuredValidator", "Passthrough", "HybridRefine" (future)
ACTION_RESOLVER_TYPE = "LLMResolver"  # Start with the current LLM-based logic

# Event Perception Model (How agents receive events)
# Options: "DirectDispatch", "SummaryContext" (old way)
EVENT_PERCEPTION_MODEL = "DirectEventDispatcher"

NARRATIVE_GOAL = "Create a humorous story about two strangers with conflicting goals who must eventually cooperate."
# DIRECTOR_COOLDOWN_MIN = 2 # Example: Could make cooldown configurable
# DIRECTOR_COOLDOWN_MAX = 5

# agent_configs = [
#     {"name": "Alice",
#      "personality": "creative, adventurous, slightly mischievous, believes in supernatural phenomena",
#      "gender": "woman",
#      "initial_location": "Forest Edge",
#      "initial_goals": ["Find magical ingredients in the forest for a special potion",
#                        "Convince someone to help explore the deeper parts of the forest",
#                        "Establish a comfortable base camp"]},

#     {"name": "Bob",
#      "personality": "practical, skeptical, organized, values comfort and security above all",
#      "gender": "man",
#      "initial_location": "Forest Edge",
#     "initial_goals": ["Find a reliable source of food and water."]}
# ]
agent_configs = [
    # --- The Investigator/Observer ---
    {
        "name": "Alice",
        "personality": "An observant and creative individual fascinated by puzzles and hidden motives, driven by a mix of fear and morbid curiosity to uncover the truth.",
        "gender": "woman",
        "initial_location": "Shelter",
        "initial_goals": [  # Goals AFTER discovering the body
            "Figure out what *really* happened to David.",
            "Observe everyone's reactions and inconsistencies closely.",
            "Look for any unusual clues.",
            "Ensure my own safety without appearing overly suspicious."
        ],
        "background": [
            "You are Alice. A storm forced you into this shelter with Bob, Charlie, Eve, and David. ",
            "David has been found murdered. You are trapped with the others, one of whom is the killer. Your goal is to figure out who, observing carefully.",
        ],
    },

    # --- The Secret Murderer ---
    {
        "name": "Bob",
        "personality": "Appears practical and safety-focused but is secretly ruthless and desperate, skillfully acting innocent to conceal his guilt as the murderer.",
        "gender": "man",
        "initial_location": "Shelter",
        "initial_goals": [  # Goals AFTER the murder (HIS secret goals)
            "Convincingly act shocked and scared about David's death.",
            "Avoid suspicion at all costs; deflect questions smoothly.",
            "Subtly steer suspicion towards someone else if possible.",
            "Find any opportunity to destroy evidence or escape.",
            "Feigned Goal: Cooperate to appear innocent."
        ],
        "background": [
            "You are Bob. You killed David in this shelter shortly before he was discovered. No one saw you. ",
            "You are trapped with Alice, Charlie, and Eve. You MUST appear innocent; act shocked and scared, but inwardly focus on deflecting suspicion and surviving."
        ],
    },

    # --- The Logical Analyst ---
    {
        "name": "Charlie",
        "personality": "A calm and analytical thinker who approaches the crisis with logical precision, focusing on systematically gathering evidence to identify the killer.",
        "gender": "man",
        "initial_location": "Shelter",
        "initial_goals": [  # Goals AFTER discovering the body
            "Establish the basic facts: time, cause, potential motives.",
            "Question everyone systematically.",
            "Secure the immediate area.",
            "Identify the most logical suspect based on opportunity/motive."
        ],
        "background": [
            "You are Charlie. A storm trapped you in this shelter with Alice, Bob, Eve, and David, who has just been found murdered. Rely on logic and observation to systematically investigate and find the killer before panic takes over."
        ],
    },

    # --- The Empathetic/Nervous One ---
    {
        "name": "Eve",
        "personality": "A highly empathetic and anxious person who reacts emotionally to the tense situation, seeking safety while trying to read others' feelings for clues.",
        "gender": "woman",
        "initial_location": "Shelter",
        "initial_goals": [  # Goals AFTER discovering the body
            "Find out who the killer is immediately so I can feel safe.",
            "Try to understand how everyone is feeling; look for emotional tells.",
            "Seek protection or ally with someone trustworthy.",
            "Express fear and urge action."
        ],
        "background": [
            "You are Eve. A storm trapped you in this shelter with Alice, Bob, Charlie, and David - but David has been murdered! You are terrified and trapped with the killer; focus on reading emotions, finding who seems suspicious, and staying safe."
        ],
    }
]
SIMULATION_MODE = 'debug'


```
============================== END FILE: src_GM\config.py ==============================


============================== START FILE: src_GM\director.py ==============================
 ```
python

# src/director.py
import google.generativeai as genai
import config
import random # For potential randomness or choosing not to act

class Director:
    """
    Observes the world state, considers a narrative goal, and uses an LLM
    to suggest and enact *indirect* environmental interventions.
    Inspired by the Game Master concept.
    """
    def __init__(self, world_state, model, narrative_goal):
        self.world = world_state # Reference to the world state
        self.llm = model         # The LLM instance for decision making
        self.narrative_goal = narrative_goal
        self.intervention_cooldown = 0 # Steps until next potential intervention
        self.steps_since_last_intervention = 0
        print(f"Director initialized with goal: '{self.narrative_goal}'")

    def observe(self):
        """Gathers the necessary information from the world state for the LLM."""
        # Get a concise summary of the current state
        agent_locations = self.world.agent_locations
        recent_events = self.world.event_log[-5:]  # Last 5 events
        weather = self.world.global_context.get('weather', 'unknown')

        observation = f"Current State Summary:\n"
        observation += f"- Weather: {weather}\n"
        observation += f"- Agent Locations: {agent_locations}\n"
        observation += f"- Recent Events:\n"
        if recent_events:
            for event in recent_events:
                # Check if the event object still has these attributes (it should)
                # Event = namedtuple("Event", ["description", "location", "scope", "step", "triggered_by"])
                location_str = event.location or 'Global'
                scope_str = event.scope or 'UnknownScope'
                triggered_by_str = event.triggered_by or 'UnknownTrigger'
                # Added check for attribute existence just in case, though they should be there
                desc_str = getattr(event, 'description', 'No description')

                # Updated prefix formatting for clarity and robustness
                prefix = f"[S{event.step} {triggered_by_str} @ {location_str}/{scope_str}]"
                observation += f"    - {prefix} {desc_str}\n"  # Use desc_str
        else:
            observation += "    - Nothing noteworthy recently in event log.\n"

        # Add goal progress check (simple example: are agents together?)
        agents_in_same_location = False
        if len(agent_locations) > 1:
            first_loc = next(iter(agent_locations.values()))
            if all(loc == first_loc for loc in agent_locations.values()):
                agents_in_same_location = True
        observation += f"- Goal Status ({self.narrative_goal}): Agents currently {'together' if agents_in_same_location else 'apart'}.\n"

        return observation.strip()

    def think(self, observation):
        """Uses the LLM to decide on an environmental intervention."""

        # Simple cooldown mechanism
        if self.steps_since_last_intervention < self.intervention_cooldown:
             self.steps_since_last_intervention += 1
             print("[Director Thinking]: Cooldown active.")
             return "ACTION: Do nothing" # Skip thinking

        prompt = f"""You are the Director/Game Master of a simulation.
Your objective is to subtly guide the narrative towards: '{self.narrative_goal}'.
You do this *only* by manipulating the environment or introducing external events.
You CANNOT directly control agents, read their minds, or change their internal states.

Current Simulation State:
{observation}

Allowed Actions:
1. Change weather: Specify the new weather condition (e.g., Sunny, Rainy, Windy, Foggy, Stormy).
2. Create ambient event: Describe a sensory event (e.g., a distant sound, a strange smell, a sudden chill).
3. Do nothing: If the situation doesn't require intervention or is progressing well.

Based on the current state and your objective, suggest ONE action from the allowed list to subtly nudge things towards the goal. Be concise and use the format 'ACTION: [Your chosen action description]'. If doing nothing, use 'ACTION: Do nothing'.

Example suggestions:
ACTION: Change weather to Rainy
ACTION: Create ambient event 'A faint birdsong is heard from the Forest Edge.'
ACTION: Do nothing

Your suggestion:"""

        print("[Director Thinking...]")
        # print(f"--- DEBUG PROMPT for Director ---\n{prompt}\n--------------------") # Optional

        try:
            response = self.llm.generate_content(prompt)
            suggestion = response.text.strip()

            if not suggestion.startswith("ACTION:"):
                print("[Director Warning]: LLM response malformed. Defaulting to 'Do nothing'. Response:", suggestion)
                return "ACTION: Do nothing"

            print(f"[Director Suggests]: {suggestion}")
            # Reset cooldown if an action is suggested (or consider resetting only on non-'Do nothing' actions)
            if "Do nothing" not in suggestion:
                 self.steps_since_last_intervention = 0
                 self.intervention_cooldown = random.randint(2, 4) # Wait a few steps after acting
                 print(f"[Director Info]: Intervention suggested. Setting cooldown to {self.intervention_cooldown} steps.")
            else:
                 self.steps_since_last_intervention += 1 # Increment even if thinking 'do nothing' this time
            return suggestion

        except Exception as e:
            print(f"[Director Error]: LLM generation failed: {e}")
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                 print(f"[Director Safety Block]: Reason: {response.prompt_feedback.block_reason}")
            return "ACTION: Do nothing" # Fail safe

    def act(self, suggestion):
        """Parses the LLM's suggestion and applies it to the world state."""
        action_part = suggestion.replace("ACTION:", "").strip()

        if action_part.lower() == "do nothing":
            print("[Director Action]: No intervention taken.")
            return

        # Parse specific actions
        if action_part.lower().startswith("change weather to "):
            new_weather = action_part[len("change weather to "):].strip().title()
            if new_weather:
                print(f"[Director Action]: Attempting to change weather to {new_weather}")
                self.world.set_weather(new_weather, triggered_by="Director")
            else:
                print("[Director Action Error]: Invalid weather condition specified.")

        elif action_part.lower().startswith("create ambient event "):
            event_description = action_part[len("create ambient event "):].strip()
            # Add quotes or formatting for clarity if needed
            if not event_description.startswith("'"): event_description = f"'{event_description}'"

            if event_description:
                scope = 'global'
                print(f"[Director Action]: Creating {scope} ambient event at '{'Global'}': {event_description}")
                self.world.log_event(f"[Ambient]: {event_description}",
                                       scope=scope,
                                       location='Global',
                                       triggered_by="Director")
            else:
                print("[Director Action Error]: Invalid ambient event description.")
        else:
            print(f"[Director Action Warning]: Could not parse suggested action: '{action_part}'")

    def step(self):
        """Perform one cycle of the Director's operation."""
        print("\n--- Director Phase ---")
        observation = self.observe()
        suggestion = self.think(observation)
        self.act(suggestion)
        print("--- End Director Phase ---")

```
============================== END FILE: src_GM\director.py ==============================


============================== START FILE: src_GM\event_dispatcher.py ==============================
 ```
python

# src_GM/event_dispatcher.py
from abc import ABC, abstractmethod
from typing import List, Dict

from world import Event  # Import the Event namedtuple
from agent.agent import Agent  # Import the Agent class
import config

class BaseEventDispatcher(ABC):
    """
    Abstract Base Class for strategies that determine which agents
    should perceive an event and deliver it to them.
    """

    @abstractmethod
    def dispatch_event(self, event: Event, registered_agents: Dict[str, Agent], agent_locations: Dict[str, str]) -> List[str]:
        """
        Determines which agents perceive the event and calls their perceive method.

        Args:
            event: The Event object to dispatch.
            registered_agents: A dictionary mapping agent names to Agent objects.
            agent_locations: A dictionary mapping agent names to their current location strings.

        Returns:
            A list of agent names to whom the event was successfully dispatched.
        """
        pass


class DirectEventDispatcher(BaseEventDispatcher):
    """
    Dispatches events based on scope and agent location.
    - Global events go to everyone.
    - Local events go to agents at that location.
    - Action outcomes go to agents at that location.
    (Based on the logic previously in WorldState.log_event)
    """

    def dispatch_event(self, event: Event, registered_agents: Dict[str, Agent], agent_locations: Dict[str, str]) -> List[str]:
        dispatched_to = []
        # Log processing start
        if config.SIMULATION_MODE == 'debug':
            print(
            f"[Dispatcher '{type(self).__name__}']: Processing event: {event.scope} @ {event.location or 'Global'} - '{event.description[:50]}...'")

        for agent_name, agent_obj in registered_agents.items():
            agent_current_loc = agent_locations.get(agent_name)
            should_perceive = False

            # Determine if the agent should perceive based on scope and location
            if event.scope == 'global':
                should_perceive = True
            elif event.location == agent_current_loc:  # Check if agent is at the event location
                if event.scope == 'local':
                    should_perceive = True
                elif event.scope == 'action_outcome':
                    # Original logic: Dispatch action outcome even to the agent who performed it.
                    # The agent's memory/processing can decide how to handle it (e.g., ignore if redundant).
                    # If filtering is desired *here*, uncomment the check below:
                    # if event.triggered_by != agent_name:
                    #    should_perceive = True
                    should_perceive = True  # Current: Dispatch action outcome to all at location

            # If perception criteria met, attempt to call agent's perceive method
            if should_perceive:
                try:
                    # print(f"[Dispatcher]: Attempting to dispatch to {agent_name}...") # Debug
                    # Call the agent's perception handler
                    agent_obj.perceive(event)
                    dispatched_to.append(agent_name)
                except AttributeError:
                    print(
                        f"[Dispatcher Error]: Agent {agent_name} object lacks 'perceive' method!")
                except Exception as e:
                    print(
                        f"[Dispatcher Error]: Failed during perceive call for {agent_name}: {e}")

        if dispatched_to:
            if config.SIMULATION_MODE == 'debug':
                print(
                f"[Dispatcher '{type(self).__name__}']: Event dispatched to: {dispatched_to}")
        # else:
            # print(f"[Dispatcher '{type(self).__name__}']: Event not dispatched to any agents based on rules.") # Can be verbose

        return dispatched_to


```
============================== END FILE: src_GM\event_dispatcher.py ==============================


============================== START FILE: src_GM\main.py ==============================
 ```
python

# --- Imports ---
from collections import namedtuple  # For creating simple Event objects
import time  # For pausing execution (e.g., between agent actions)
from typing import List  # For type hinting lists (e.g., list of Agents)
import google.generativeai as genai  # Google's Generative AI library
import argparse  # For parsing command-line arguments

import config  # Import the whole config module to access global settings

# Import custom modules for simulation components
from world import WorldState
from agent.agent import Agent
from director import Director

# --- Data Structures ---

# Define a simple structure to represent events within the simulation
Event = namedtuple(
    "Event", ["description", "location", "scope", "step", "triggered_by"])

# --- Factory Functions for Components ---
# These functions allow creating different implementations of simulation components
# based on configuration strings, promoting modularity.


def get_memory_module(agent, memory_type):
    """Factory function to create an agent's memory module."""
    if memory_type == "SimpleMemory":
        from agent.memory import SimpleMemory
        return SimpleMemory()
    if memory_type == "ShortLongTMemory":
        from agent.memory import ShortLongTMemory
        # Example of passing configuration to a specific memory type
        return ShortLongTMemory(agent, reflection_threshold=10)
    else:
        # Handle unknown memory types specified in config
        raise ValueError(f"Unknown memory type: {memory_type}")


def get_planning_module(planning_type, model):
    """Factory function to create an agent's planning module (thinker)."""
    if planning_type == "GeminiThinker":
        from agent.planning import SimplePlanning
        return SimplePlanning(model)
    # Add other planning types here if needed
    # elif planning_type == "AnotherThinker":
    #     from agent.planning import AnotherThinker
    #     return AnotherThinker(...)
    else:
        # Handle unknown planning types specified in config
        raise ValueError(f"Unknown thinker type: {planning_type}")


def get_action_resolver(resolver_type, model, world_ref=None):
    """Factory function to create the action resolver."""
    if resolver_type == "LLMResolver":
        from action_resolver import LLMActionResolver
        # Pass the language model and a reference to the world state
        return LLMActionResolver(model, world_ref)
    # Add other resolver types here if needed
    # elif resolver_type == "RuleBasedResolver":
    #     from action_resolver import RuleBasedResolver
    #     return RuleBasedResolver(...)
    else:
        # Handle unknown resolver types specified in config
        raise ValueError(f"Unknown action resolver type: {resolver_type}")


def get_event_dispatcher(dispatcher_type: str):
    """Factory function to create the event dispatcher."""
    if dispatcher_type == "DirectEventDispatcher":
        from event_dispatcher import DirectEventDispatcher
        return DirectEventDispatcher()
    # Add other dispatcher types here if needed
    # elif dispatcher_type == "FilteredEventDispatcher":
    #     from event_dispatcher import FilteredEventDispatcher
    #     return FilteredEventDispatcher(...)
    else:
        # Handle unknown dispatcher types specified in config
        raise ValueError(f"Unknown event dispatcher type: {dispatcher_type}")

# --- Main Simulation Function ---


def run_simulation():
    """
    Sets up and runs the agent simulation loop.
    Initializes the world, agents, director, and other components based on the 'config' module.
    Manages the main simulation steps, including agent thinking, action resolution,
    world updates, event dispatching, and user interaction.
    Output verbosity depends on the config.SIMULATION_MODE ('debug' or 'story').
    """

    # --- Initialization Phase ---
    # Configure logging based on the simulation mode set in config
    if config.SIMULATION_MODE == 'debug':
        print("--- Starting Agent Simulation with Director (DEBUG MODE) ---")
        print(f"Config: Memory={config.AGENT_MEMORY_TYPE}, Thinker={config.AGENT_PLANNING_TYPE}, Resolver={config.ACTION_RESOLVER_TYPE}, Perception={config.EVENT_PERCEPTION_MODEL}")
    elif config.SIMULATION_MODE == 'story':
        print("--- Starting Agent Simulation ---")

    # 1. Initialize LLM Model
    if config.SIMULATION_MODE == 'debug':
        print(f"Configuring Gemini model: {config.MODEL_NAME}")
    # Configure the generative AI model using the API key from config
    genai.configure(api_key=config.GEMINI_API_KEY)
    # Create the specific generative model instance
    model = genai.GenerativeModel(
        model_name=config.MODEL_NAME,
        generation_config=config.GENERATION_CONFIG,
    )
    if config.SIMULATION_MODE == 'debug':
        print("Model configured.")

    # 2. Initialize World State and Event Dispatcher
    # Create the event dispatcher using the factory function based on config
    event_dispatcher = get_event_dispatcher(config.EVENT_PERCEPTION_MODEL)
    # Create the world state with known locations from config
    world = WorldState(locations=config.KNOWN_LOCATIONS)
    # Set initial global context (e.g., weather)
    world.global_context['weather'] = "Clear"
    if config.SIMULATION_MODE == 'debug':
        print("World state and event dispatcher initialized.")

    # 3. Initialize Agents
    agents: List[Agent] = []  # Type hint for a list of Agent objects
    if config.SIMULATION_MODE == 'debug':
        print("Initializing agents...")
    # Loop through agent configurations defined in the config module
    for agent_conf in config.agent_configs:
        agent_name = agent_conf["name"]
        # Create the planning module (thinker) for the agent
        thinker = get_planning_module(config.AGENT_PLANNING_TYPE, model)
        # Create the Agent instance
        agent = Agent(
            name=agent_name,
            gender=agent_conf["gender"],
            personality=agent_conf["personality"],
            initial_goals=agent_conf["initial_goals"],
            background=agent_conf["background"],
            memory_module=None,  # Memory will be assigned below
            planning_module=thinker
        )
        # Create and assign the memory module using the factory function
        agent.memory = get_memory_module(agent, config.AGENT_MEMORY_TYPE)

        # Add the agent to the simulation's list of agents
        agents.append(agent)
        # Set the agent's starting location in the world state
        start_location = agent_conf["initial_location"]
        world.add_agent_to_location(
            agent_name, start_location, triggered_by="Setup")  # Log the setup action
        # Register the agent instance with the world (for lookups, event dispatching)
        world.register_agent(agent)
        if config.SIMULATION_MODE == 'debug':
            print(f"- Agent '{agent_name}' created at '{start_location}'.")

    if config.SIMULATION_MODE == 'debug':
        print("Agents initialized.")

    # 4. Initialize Director
    director_model = model  # The director might use the same LLM or a different one
    # Create the Director instance, passing the world, model, and narrative goal
    director = Director(world, director_model, config.NARRATIVE_GOAL)
    if config.SIMULATION_MODE == 'debug':
        print(f"Director initialized with goal: '{config.NARRATIVE_GOAL}'")

    # 5. Initialize Action Resolver
    # Create the action resolver using the factory function
    action_resolver = get_action_resolver(
        config.ACTION_RESOLVER_TYPE, model, world_ref=world)  # Pass world reference
    if config.SIMULATION_MODE == 'debug':
        print("Action resolver initialized.")

    # ---------------------------------------- Simulation Steps ----------------------------------------
    step = 0  # Initialize step counter
    # Main simulation loop, continues until max steps are reached
    while step < config.SIMULATION_MAX_STEPS:
        step += 1  # Increment step counter
        world.advance_step()  # Advance the world's internal clock/step counter

        # Print step header based on simulation mode
        if config.SIMULATION_MODE == 'debug':
            print(
                f"\n{'='*15} Simulation Step {step}/{config.SIMULATION_MAX_STEPS} {'='*15}")
            # In debug mode, print the full world state at the start of the step
            print(world.get_full_state_string())
        elif config.SIMULATION_MODE == 'story':
            print(f"\n--- TIME STEP {step} ---")

        # ---------------------------------------- Agent Thinking Phase ----------------------------------------
        # Each agent observes the world and decides on their next action/intention.
        if config.SIMULATION_MODE == 'debug':
            print("\n--- Agent Thinking Phase ---")
        agent_intentions = {}  # Store planned action/intention for each agent
        agent_current_locations = {}  # Store current location for action resolution context
        for agent in agents:
            if config.SIMULATION_MODE == 'debug':
                print(f"\n-- Processing {agent.name} --")
            # Get the agent's current location from the world state
            current_loc = world.agent_locations.get(agent.name, None)
            # Handle cases where an agent might not have a location (should ideally not happen)
            if not current_loc:
                if config.SIMULATION_MODE == 'debug':
                    print(
                        f"[Sim Warning]: Agent {agent.name} has no location! Skipping.")
                continue  # Skip this agent for this step
            agent_current_locations[agent.name] = current_loc  # Store location

            # Ask the agent to plan its action based on the current world state
            intended_output = agent.plan(world)
            # Store the intention
            agent_intentions[agent.name] = intended_output

            # Optional pause to avoid hitting API rate limits or to slow down simulation
            time.sleep(1.0)

        # ---------------------------------------- Action Resolution Phase ----------------------------------------
        # The Action Resolver interprets agent intentions and determines the outcomes
        # and necessary world state changes.
        if config.SIMULATION_MODE == 'debug':
            print("\n--- Action Resolution Phase ---")

        resolution_results = {}  # Store the outcome dict from the resolver for each agent
        all_state_updates = []  # Collect ALL state update instructions before applying them
        # Collect events resulting from actions (success or failure)
        all_outcome_events = []

        # Iterate through the intentions planned by each agent
        for agent_name, intent in agent_intentions.items():
            agent_loc = agent_current_locations.get(agent_name)
            # Ensure the agent has a location and the action resolver is available
            if agent_loc and action_resolver:
                if config.SIMULATION_MODE == 'debug':
                    print(f"-- Resolving for {agent_name} at {agent_loc} --")
                # Call the action resolver to determine the outcome of the intended action
                result = action_resolver.resolve(
                    agent_name, agent_loc, intent, world
                )
                resolution_results[agent_name] = result  # Store the raw result

                # --- Process Successful Action ---
                if result and result.get("success"):
                    outcome_desc = result.get(
                        'outcome_description', f"{agent_name} acted.")  # Default description
                    if config.SIMULATION_MODE == 'debug':
                        # Detailed debug log for successful action
                        print(
                            f"[Resolver OK] {agent_name}: {result.get('action_type')} -> {outcome_desc}")
                    if config.SIMULATION_MODE == 'story':
                        # Narrative output for successful action
                        # Add extra newline for story readability
                        print(f"{outcome_desc}\n\n")

                    # Collect world state updates if the action caused any
                    if result.get("world_state_updates"):
                        all_state_updates.extend(result["world_state_updates"])
                    # Collect the outcome event to be logged and potentially perceived
                    all_outcome_events.append(
                        # description, scope, location, triggered_by
                        (outcome_desc, 'action_outcome', agent_loc, agent_name)
                    )
                # --- Process Failed Action ---
                elif result:  # If result exists but 'success' is not True
                    reason = result.get('reasoning', 'Unknown reason')
                    outcome_desc = result.get(
                        'outcome_description', 'Action failed.')
                    if config.SIMULATION_MODE == 'debug':
                        # Detailed debug log for failed action
                        print(f"[Resolver FAIL] {agent_name}: {reason}")
                        print(f"   Outcome: {outcome_desc}")
                    if config.SIMULATION_MODE == 'story':
                        # Narrative output for failed action
                        print(
                            f"{agent_name} tried to act, but {outcome_desc.lower()}")  # Narrative phrasing

                    # Collect the failure outcome event
                    all_outcome_events.append(
                        (outcome_desc, 'action_outcome', agent_loc, agent_name)
                    )
                # --- Handle Resolver Errors ---
                else:  # If the resolver returned None or an unexpected structure
                    error_msg = f"System error resolving {agent_name}'s action."
                    if config.SIMULATION_MODE == 'debug':
                        print(
                            f"[Resolver ERROR] Critical failure resolving for {agent_name}")
                    if config.SIMULATION_MODE == 'story':
                        print(
                            f"[System Note] Issue resolving {agent_name}'s action.")
                    # Log a system error event
                    all_outcome_events.append(
                        (error_msg, 'system_error', agent_loc,
                        'System')  # Use 'System' as trigger
                    )
            # --- Handle Missing Agent Location or Resolver ---
            else:
                if config.SIMULATION_MODE == 'debug':
                    print(
                        f"[Sim Warning]: Cannot resolve action for {agent_name}, location or resolver unknown.")
            # Optional pause between resolving actions
            time.sleep(0.5)

        # ---------------------------------------- World Update Phase ----------------------------------------
        # Apply all collected state changes to the world simultaneously.
        if config.SIMULATION_MODE == 'debug':
            print("\n--- World Update Phase ---")
        if all_state_updates:
            # Apply the list of update instructions to the world state
            world.apply_state_updates(
                all_state_updates, triggered_by="AgentActions")  # Indicate the trigger
            if config.SIMULATION_MODE == 'debug':
                print("Applied world state updates.")
        else:
            # No updates were generated in this step
            if config.SIMULATION_MODE == 'debug':
                print("No world state updates required.")

        # ---------------------------------------- Agents Perceiving and Event Logging Phase ----------------------------------------
        # Log the outcomes of actions as events and dispatch them so agents can perceive them.
        if config.SIMULATION_MODE == 'debug':
            print("\n--- Logging Action Outcomes & Dispatching Events ---")
        # Iterate through the collected outcome events (successes, failures, errors)
        for desc, scope, loc, trig_by in all_outcome_events:
            # 1. Log the event to the world's historical record
            world.log_event(desc, scope, loc, trig_by)

            # 2. Create an Event object
            new_event = Event(
                description=desc,
                location=loc,
                scope=scope,
                step=world.current_step,
                triggered_by=trig_by
            )
            # 3. Dispatch the event using the configured dispatcher
            # The dispatcher determines which agents should perceive this event
            event_dispatcher.dispatch_event(
                new_event, world.registered_agents, world.agent_locations)

        # ---------------------------------------- End Step ----------------------------------------
        if config.SIMULATION_MODE == 'debug':
            print("\n--- End of Step ---")
            # Print the final world state after all updates and events for the step
            print(world.get_full_state_string())

        # ---------------------------------------- User Input ----------------------------------------
        # Allows pausing the simulation, quitting, or changing parameters mid-run.
        user_input = input(
            "Enter for next step, 'goal <new goal>' to change director goal, 'w <weather>' for weather, 'q' to quit: "
        ).lower().strip()

        if user_input == 'q':
            # Quit the simulation loop
            print("Quitting simulation by user request.")
            break
        elif user_input.startswith('goal '):
            # Change the director's narrative goal dynamically
            new_goal = user_input[len('goal '):].strip()
            if new_goal:
                print(f"Updating Director goal to: '{new_goal}'")
                director.narrative_goal = new_goal
                # Log this user command as a world event
                world.log_event(
                    f"COMMAND: Director narrative goal updated to '{new_goal}'.",
                    scope="global", location="world", triggered_by="User"
                )
            else:
                print("Invalid command. Use 'goal <description>'.")
        elif user_input.startswith('w '):
            # Manually override the weather
            # Capitalize words
            new_weather = user_input[len('w '):].strip().title()
            if new_weather:
                print(f"Manual weather override to: {new_weather}")
                # Update world state and log event
                world.set_weather(new_weather, triggered_by="User")
            else:
                print("Invalid command. Use 'w <condition>'.")
        # Handle empty input (just press Enter) or unknown commands
        elif user_input:
            if user_input:  # Only print error if it wasn't just Enter
                print(
                    f"Unknown command: '{user_input}'. Press Enter to continue.")
            # Otherwise, pressing Enter just proceeds to the next step

    # ---------------------------------------- Simulation End ----------------------------------------
    print(f"\n--- Simulation Ended after {step} steps ---")


# --- Main Execution Block ---
if __name__ == "__main__":
    # Set up command-line argument parsing
    parser = argparse.ArgumentParser(description="Run Agent Simulation")
    # Create a mutually exclusive group: user must choose --debug OR --story
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--debug', action='store_true',
                       help='Enable detailed debug logging.')
    group.add_argument('--story', action='store_true',
                       help='Enable narrative story logging.')

    # Parse the command-line arguments provided by the user
    args = parser.parse_args()

    # Set the global simulation mode in the config module based on the arguments.
    # This allows other modules imported after this point to easily check the mode.
    config.SIMULATION_MODE = 'debug' if args.debug else 'story'

    # Call the main simulation function, which will now use the mode set in config
    run_simulation()


```
============================== END FILE: src_GM\main.py ==============================


============================== START FILE: src_GM\world.py ==============================
 ```
python

# src/world.py
from typing import Dict
import config
from collections import namedtuple
from agent.agent import Agent

# Define a structure for events for clarity
Event = namedtuple(
    "Event", ["description", "location", "scope", "step", "triggered_by"]
)


class WorldState:
    def __init__(self, locations):
        self.agent_locations = {}  # agent_name -> location_name
        self.location_descriptions = locations
        self.location_connectivity = {  # Defines possible direct movements
            "Park": ["Shelter", "Forest Edge"],
            "Shelter": ["Park"],
            "Forest Edge": ["Park"],
            # Add more connections as locations are added
        }
        self.location_properties = {  # Defines states/features of locations
            "Park": {"ground": "grassy"},
            # Example: Shelter door starts unlocked
            "Shelter": {"door_locked": False, "contains": []},
            "Forest Edge": {"terrain": "uneven"},
        }
        self.global_context = {"weather": "Clear"}
        self.event_log = []  # Now stores Event tuples
        self.current_step = 0  # Track simulation step

        self.registered_agents: Dict[str, Agent] = {}  # Added type hint
        # Store the passed dispatcher instance

    def register_agent(self, agent: Agent):
        """Registers an agent to receive events."""
        if agent.name not in self.registered_agents:
            self.registered_agents[agent.name] = agent
            print(f"[World Event Update]: Registered {agent.name} for events.")

    def unregister_agent(self, agent_name):
        """Unregisters an agent."""
        if agent_name in self.registered_agents:
            del self.registered_agents[agent_name]
            print(f"[World Event Update]: Unregistered {agent_name}.")

    def advance_step(self):
        self.current_step += 1

    # --- Helper methods to access rules ---
    def get_reachable_locations(self, from_location):
        """Returns list of locations directly reachable from the given one."""
        return self.location_connectivity.get(from_location, [])

    def get_location_property(self, location, prop_name):
        """Safely gets a property of a location."""
        return self.location_properties.get(location, {}).get(prop_name, None)

    def set_location_property(self, location, prop_name, value, triggered_by="System"):
        """Sets a property, potentially logging an event via log_event."""
        if location not in self.location_properties:
            print(
                f"[World State Warning]: Location '{location}' not found for setting property."
            )
            return False
        if prop_name not in self.location_properties[location]:
            print(
                f"[World State Warning]: Property '{prop_name}' doesn't exist for '{location}'. Adding it."
            )
            # Decide if dynamic property creation is allowed or should error

        old_value = self.location_properties[location].get(prop_name, "None")
        if old_value != value:
            self.location_properties[location][prop_name] = value
            # Log the change as an event? Depends on granularity needed.
            # Example: log the *effect* rather than the state change itself.
            # e.g., if setting door_locked=False, the event might be "The shelter door unlocks"
            print(
                f"[World State Update]: Property '{prop_name}' of '{location}' changed to '{value}' (Trigger: {triggered_by})."
            )
            return True
        return False

    # --- End Helpers ---

    def add_agent_to_location(self, agent_name, location_name, triggered_by="Setup"):
        """Adds agent and updates location state if needed."""
        if location_name in self.location_descriptions:
            old_location = self.agent_locations.get(agent_name)
            self.agent_locations[agent_name] = location_name
            print(f"World: Agent {agent_name} now at {location_name}")

            # Update 'contains' property if location has it
            if old_location and old_location != location_name:
                if "contains" in self.location_properties.get(old_location, {}):
                    if agent_name in self.location_properties[old_location]["contains"]:
                        self.location_properties[old_location]["contains"].remove(
                            agent_name
                        )

            if "contains" in self.location_properties.get(location_name, {}):
                if (
                    agent_name
                    not in self.location_properties[location_name]["contains"]
                ):
                    self.location_properties[location_name]["contains"].append(
                        agent_name
                    )

            # Log the arrival event (could be refined based on triggered_by)
            if triggered_by != "Setup":  # Don't log redundant 'appears' if moving
                self.log_event(
                    f"{agent_name} arrives.",
                    scope="local",
                    location=location_name,
                    triggered_by=triggered_by,
                )
            elif not old_location:  # Log initial appearance
                self.log_event(
                    f"{agent_name} appears in the {location_name}.",
                    scope="local",
                    location=location_name,
                    triggered_by=triggered_by,
                )
        else:
            print(
                f"Warning: Cannot move {agent_name} to unknown location '{location_name}'"
            )

    def get_agents_at(self, location_name):
        # Add simple check if location exists
        if location_name not in self.location_descriptions:
            return []
        return [
            name for name, loc in self.agent_locations.items() if loc == location_name
        ]

    def log_event(
        self,
        description,
        scope="local",
        location=None,
        triggered_by="Simulation",
        dispatch=True,
    ):
        """Logs an event"""
        new_event = Event(
            description=description,
            location=location,
            scope=scope,
            step=self.current_step,
            triggered_by=triggered_by,
        )
        self.event_log.append(new_event)
        log_prefix = f"[Event Log Step {self.current_step}][{triggered_by} @ {location or 'Global'}/{scope}]"
        if config.SIMULATION_MODE == "debug":
            print(f"{log_prefix}: {description}")

        # Trim log if needed
        if (
            len(self.event_log) > config.MAX_RECENT_EVENTS * 2
        ):  # Keep a longer internal log
            self.event_log.pop(0)

    def set_weather(self, new_weather, triggered_by="Simulation"):
        """Changes the weather and logs the event."""
        old_weather = self.global_context.get("weather", "unknown")
        if old_weather != new_weather:
            self.global_context["weather"] = new_weather
            # Log weather change as a GLOBAL event
            self.log_event(
                f"The weather changes from {old_weather} to {new_weather}.",
                scope="global",
                location=None,  # Global event has no specific location
                triggered_by=triggered_by,
            )
            return True
        return False

    def get_static_context_for_agent(self, agent_name):
        """Provides minimal, relatively static context."""
        location = self.agent_locations.get(agent_name)
        if not location:
            return "You are lost."

        context = f"Current Location: {location} ({self.location_descriptions.get(location, 'Unknown')}).\n"
        context += (
            f"Current Weather: {self.global_context.get('weather', 'Unknown')}.\n"
        )
        exits = self.get_reachable_locations(location)
        context += f"Visible Exits: {exits if exits else 'None'}.\n"

        # Add location properties that are visible/relevant
        location_props = self.location_properties.get(location, {})
        visible_props = []
        if "ground" in location_props:
            visible_props.append(f"The ground is {location_props['ground']}")
        if "terrain" in location_props:
            visible_props.append(f"The terrain is {location_props['terrain']}")
        if "door_locked" in location_props:
            visible_props.append(
                f"The door is {'locked' if location_props['door_locked'] else 'unlocked'}"
            )
        if visible_props:
            context += f"Location Features: {'. '.join(visible_props)}.\n"

        # Add information about other agents in the location
        other_agents = [
            name for name in self.get_agents_at(location) if name != agent_name
        ]
        if other_agents:
            context += f"Other agents present: {', '.join(other_agents)}.\n"
        else:
            context += "You are alone here.\n"

        # Add information about items in the location
        if "contains" in location_props and location_props["contains"]:
            items = location_props["contains"]
            if isinstance(items, list) and items:
                context += f"You can see: {', '.join(items)}.\n"

        return context

    def get_full_state_string(self):
        """For debugging - shows more structured log."""
        state = f"--- World State (Step: {self.current_step}) ---\n"
        state += f"Global Context: {self.global_context}\n"
        state += f"Agent Locations: {self.agent_locations}\n"
        state += f"Location Properties: {self.location_properties}\n"
        # Show who listens
        state += f"Registered Agents: {list(self.registered_agents.keys())}\n"
        state += f"Event Log ({len(self.event_log)} total, showing last {config.MAX_RECENT_EVENTS}):\n"
        display_events = self.event_log[-config.MAX_RECENT_EVENTS :]
        for event in display_events:
            state += f"  - St{event.step} [{event.triggered_by}@{event.location or 'Global'}/{event.scope}] {event.description}\n"
        return state + "-------------------"

    def apply_state_updates(self, updates: list, triggered_by: str):
        """Applies a list of state changes suggested by the Action Resolver."""
        if not updates:
            return

        print(
            f"[World State Apply]: Applying {len(updates)} updates triggered by {triggered_by}."
        )
        for update in updates:
            try:
                update_type = update[0]
                target = update[1]
                value = update[2]

                if update_type == "agent_location":
                    # Value is the new location name
                    self.add_agent_to_location(
                        agent_name=target,
                        location_name=value,
                        triggered_by=triggered_by,
                    )
                elif update_type == "location_property":
                    # Target is location name, value is prop_name, need 4th element for prop_value
                    if len(update) == 4:
                        prop_name = value
                        prop_value = update[3]
                        self.set_location_property(
                            location=target,
                            prop_name=prop_name,
                            value=prop_value,
                            triggered_by=triggered_by,
                        )
                    else:
                        print(
                            f"[World State Apply Error]: Invalid format for location_property update: {update}"
                        )
                # Add more update types here (e.g., global context, agent inventory)
                else:
                    print(
                        f"[World State Apply Warning]: Unknown update type '{update_type}'"
                    )

            except IndexError as e:
                print(
                    f"[World State Apply Error]: Malformed update tuple {update}: {e}"
                )
            except Exception as e:
                print(
                    f"[World State Apply Error]: Failed to apply update {update}: {e}"
                )


```
============================== END FILE: src_GM\world.py ==============================


============================== START FILE: src_GM\agent\agent.py ==============================
 ```
python

# agent.py
from agent.memory import BaseMemory
from agent.planning import BasePlanning
# from world import WorldState
import config
class Agent:
    def __init__(self, name:str,gender:str, personality:str, memory_module:BaseMemory, planning_module:BasePlanning, initial_goals: list[str] = None,background:list[str] = None):
        self.name = name
        self.gender = gender
        self.personality = personality
        self.memory = memory_module # An instance of BaseMemory
        self.planning = planning_module # An instance of BasePlanning
        self.goals = initial_goals if initial_goals is not None else [] # List of goal descriptions
        self.background = background if background is not None else []  # Placeholder for agent's background
        self.action_buffer = None  # Store the output of plan() before resolution
        print(f"Agent {name} initialized with {type(memory_module).__name__} and {type(planning_module).__name__}.")

    def perceive(self, event):
        """Processes a perceived event from the world and stores it in memory."""
        # Simple formatting for now, could be more sophisticated
        perception_text = f"[Perception @ Step {event.step}] ({event.scope} at {event.location or 'Global'} by {event.triggered_by}): {event.description}"
        self.memory.add_observation(perception_text)
        if config.SIMULATION_MODE == 'debug':
            print(f"DEBUG {self.name} Perceived: {perception_text}") # Optional debug

    def add_goal(self, goal_description: str):
        """Adds a new goal to the agent's list."""
        if goal_description not in self.goals:
            self.goals.append(goal_description)
            if config.SIMULATION_MODE == 'debug':
                print(f"DEBUG {self.name} added goal: {goal_description}")
            # Optionally, add this event to memory
            self.memory.add_observation(f"[Internal] Added new goal: {goal_description}")

    def plan(self, world_state):
        """
        Agent's thinking cycle. Uses memory, goals, and planning to decide next action intent.
        Does NOT execute the action, just returns the intended output.
        """
        # 1. Get memory context
        memory_context = self.memory.get_memory_context()

        # 2. Get minimal static world context (if needed by the thinker)
        static_context = world_state.get_static_context_for_agent(self.name)

        # 3. Plan (call the planning module)
        # Pass agent reference (for personality/goals), static context, and memory context
        action_output = self.planning.generate_output(
            self, static_context, memory_context)

        # 4. Store intended action (important!)
        self.action_buffer = action_output
        # Also add own *intended* action to memory for self-reflection
        self.memory.add_observation(
            f"[My Intent @ Step {world_state.current_step}] {action_output}")

        return action_output

    # Note: Memory update for the *result* of the action now happens implicitly
    # when the ActionResolver's outcome event is logged and dispatched back
    # to the agent via perceive().


```
============================== END FILE: src_GM\agent\agent.py ==============================


============================== START FILE: src_GM\agent\memory.py ==============================
 ```
python

# memory.py
import config
import google.generativeai as genai  # Add this import
from abc import ABC, abstractmethod 
from typing import TYPE_CHECKING, Optional, List, Dict, Any
if TYPE_CHECKING:
   from agent import Agent 

# --- Base Memory Class ---
class BaseMemory(ABC):
    """Abstract base class for agent memory modules."""
    def __init__(self, agent: 'Agent'):
        """Initializes the memory module, linking it to its agent."""
        self.agent = agent # Store the agent reference

    @abstractmethod
    def add_observation(self, observation_text: str, step: Optional[int] = None, type: str = "Generic"):
        """Adds a piece of information to memory.

        Args:
            observation_text: The core text of the memory.
            step: The simulation step number when the observation occurred (optional).
            type: The type of memory (e.g., "Perception", "Intent", "Dialogue", "Reflection").
        """
        pass

    @abstractmethod
    def get_memory_context(self, **kwargs) -> str:
        """Returns a string summary of relevant memories for the LLM prompt.
           Accepts optional keyword arguments for more specific retrieval if implemented.
        """
        pass

    @abstractmethod
    def clear(self):
        """Clears the memory."""
        pass

class SimpleMemory(BaseMemory):
    """A basic rolling string buffer memory."""
    def __init__(self, agent: 'Agent', max_length: int = config.MAX_MEMORY_TOKENS):
        """Initializes SimpleMemory."""
        super().__init__(agent) 
        self.memory_buffer = ""
        self.max_length = max_length # Approximate character length

    def add_observation(self, observation_text: str, step: Optional[int] = None, type: str = "Generic"):
        """Adds observation text to the buffer, prepending type/step if available."""
        # Format the entry with available metadata
        prefix = f"[T:{type}" + (f" S:{step}" if step is not None else "") + "] "
        new_entry = prefix + observation_text.strip()
        
        # Add new observation, ensuring separation
        # Don't overwrite new_entry here
        if self.memory_buffer:
            self.memory_buffer = f"{self.memory_buffer}\n{new_entry}"
        else:
            self.memory_buffer = new_entry
            
        # Trim if exceeds max length (simple truncation from the beginning)
        if len(self.memory_buffer) > self.max_length:
            excess = len(self.memory_buffer) - self.max_length
            # Try to cut off at a newline to keep entries somewhat intact
            first_newline = self.memory_buffer.find('\n', excess)
            if first_newline != -1:
                 self.memory_buffer = self.memory_buffer[first_newline+1:]
            else: # If no newline found after excess, just truncate
                 self.memory_buffer = self.memory_buffer[excess:]
        if config.SIMULATION_MODE == 'debug':
            print(f"DEBUG Memory Add: Added '{new_entry[:50]}...'. Buffer size: {len(self.memory_buffer)}") # Debug

    def get_memory_context(self, **kwargs) -> str:
        """Returns the entire (potentially trimmed) memory buffer."""
        if not self.memory_buffer:
            return "No specific memories recalled."
        return f"Recollections (most recent last):\n{self.memory_buffer}"

    def clear(self):
        self.memory_buffer = ""

# --- Short-Long Term Memory with Reflection ---
class ShortLongTMemory(BaseMemory):
    """Memory storing recent events (short-term) and LLM-generated
       reflections/summaries (long-term). Does NOT use embeddings."""
    def __init__(self, agent: 'Agent', reflection_threshold: int = 5):
        """
        Initializes ShortLongTermMemory.

        Args:
            agent: The agent this memory belongs to.
            reflection_threshold: Number of new short-term memories needed to trigger a reflection.
        """
        super().__init__(agent)
        self.short_term_memory: List[Dict[str, Any]] = [] # Stores {'step': int, 'type': str, 'text': str}
        self.long_term_memory: List[str] = [] # Stores reflection strings
        self.reflection_threshold = reflection_threshold
        self.unreflected_count = 0 # Counter for triggering reflection

        # Configure and instantiate the reflection model
        self.reflection_model = None  # Initialize to None
        try:
            # Configure and instantiate a dedicated model 
            generation_config = {
                    "temperature": 0.7, # Slightly less random for reflections
                    "top_p": 0.9,
                    "top_k": 40,
                    "max_output_tokens": 256 # Limit reflection length if needed
                }
            
            # Fix the variable name from reflection_mode to reflection_model
            self.reflection_model = genai.GenerativeModel(
                model_name=config.MODEL_NAME, # Or a specific model if desired
                generation_config=generation_config,
            )
            print(f"DEBUG {self.agent.name}: Reflection model '{config.MODEL_NAME}' initialized for ShortLongTermMemory.")
        except Exception as e:
            print(f"ERROR {self.agent.name}: Failed to initialize reflection model '{config.MODEL_NAME}': {e}. Reflections will be disabled.")
        

    def add_observation(self, observation_text: str, step: Optional[int] = None, type: str = "Generic"):
        """Adds observation to short-term memory and triggers reflection if threshold is met."""
        memory_entry = {
            "step": step,
            "type": type,
            "text": observation_text.strip()
        }
        self.short_term_memory.append(memory_entry)
        self.unreflected_count += 1
        # print(f"DEBUG {self.agent.name} Memory Add ShortTerm: Added '{memory_entry['text'][:50]}...'. Unreflected: {self.unreflected_count}")

        # --- Trigger Reflection ---
        if self.reflection_model and self.unreflected_count >= self.reflection_threshold:
            self._reflect()
            self.unreflected_count = 0 # Reset counter after reflection

    def _reflect(self):
        """Generates and stores a long-term reflection based on recent short-term memories."""
        if not self.reflection_model:
            print(f"DEBUG {self.agent.name}: Skipping reflection, model not available.")
            return
        if len(self.short_term_memory) < self.reflection_threshold:
            # Should not happen if called correctly, but safety check
            return

        # Get the most recent memories that haven't been reflected upon yet
        # For simplicity, we take the last 'reflection_threshold' entries.
        # A more robust way might track indices, but this works for now.
        memories_to_reflect = self.short_term_memory[-self.reflection_threshold:]

        # --- Prepare Prompt for Reflection LLM ---
        # Basic agent context
        prompt_context = f"Agent Name: {self.agent.name}\n"
        prompt_context += f"Goals: {self.agent.goals}\n"
        prompt_context += f"Personality: {self.agent.personality}\n"
        prompt_context += f"Background: {self.agent.background}\n\n"
        prompt_context += "Recent events and thoughts:\n"

        # Format the memories for the prompt
        for mem in memories_to_reflect:
            prefix = f"[T:{mem['type']}" + (f" S:{mem['step']}" if mem['step'] is not None else "") + "]"
            prompt_context += f"{prefix} {mem['text']}\n"

        # Reflection Instruction
        prompt_instruction = (
            "\nBased on the agent's personality and the recent events listed above, "
            "what are 1-3 high-level insights, conclusions, important observations, "
            "or summaries about the current situation, relationships, or goals? "
            "Focus on significance and synthesis, not just listing the events. Be concise."
        )

        full_prompt = prompt_context + prompt_instruction
        if config.SIMULATION_MODE == 'debug':
            print(f"DEBUG {self.agent.name}: Generating reflection...")
            print(f"--- Reflection Prompt ---\n{full_prompt}\n-----------------------") # Uncomment for deep debug

        # --- Call LLM for Reflection ---
        try:
            response = self.reflection_model.generate_content(full_prompt)
            reflection_text = response.text.strip()

            if reflection_text:
                self.long_term_memory.append(reflection_text)
                if config.SIMULATION_MODE == 'debug':
                    print(f"DEBUG {self.agent.name} Reflection Added: '{reflection_text[:80]}...'")
            else:
                print(f"WARN {self.agent.name}: Reflection generated empty text.")

        except Exception as e:
            print(f"ERROR {self.agent.name}: Failed to generate reflection: {e}")
            # Optionally add a placeholder LTM entry indicating failure?

    def get_memory_context(self, **kwargs) -> str:
        """Returns a formatted string containing both long-term reflections
           and recent short-term observations."""

        context = "--- Core Reflections & Summaries ---\n"
        if self.long_term_memory:
            # Maybe limit the number of reflections shown? For now, show all.
            context += "\n".join(f"- {ltm}" for ltm in self.long_term_memory) + "\n"
        else:
            context += "No long-term reflections generated yet.\n"

        context += "\n--- Recent Observations (most recent last) ---\n"
        if self.short_term_memory:
            # Limit the number of short-term memories shown in context?
            max_short_term_in_context = kwargs.get('max_short_term_entries', 20) # Example limit
            start_index = max(0, len(self.short_term_memory) - max_short_term_in_context)
            if start_index > 0:
                context += "[...older observations omitted...]\n"

            for mem in self.short_term_memory[start_index:]:
                prefix = f"[T:{mem['type']}" + (f" S:{mem['step']}" if mem['step'] is not None else "") + "]"
                context += f"{prefix} {mem['text']}\n"
        else:
            context += "No recent observations recorded.\n"

        # Limit total context length if needed (crude truncation)
        max_total_length = kwargs.get('max_context_length', 4000) # Example limit
        if len(context) > max_total_length:
            context = f"... (Memory Context Trimmed) ...\n{context[-max_total_length:]}"

        # print(f"DEBUG {self.agent.name} Memory Context Requested. Length: {len(context)}")
        return context.strip()
    def clear(self):
        """Clears both short-term and long-term memory."""
        self.short_term_memory = []
        self.long_term_memory = []
        self.unreflected_count = 0
        print(f"DEBUG {self.agent.name}: Memory cleared.")
        

    

    

    

```
============================== END FILE: src_GM\agent\memory.py ==============================


============================== START FILE: src_GM\agent\planning.py ==============================
 ```
python

# planning.py
import google.generativeai as genai
import config
from abc import ABC, abstractmethod

class BasePlanning(ABC):
    """Abstract base class for agent thinking/decision-making modules."""
    @abstractmethod
    def generate_output(self, agent, static_world_context, memory_context):  # Renamed
        """Generates the agent's next intended action/thought/speech output."""
        pass

class SimplePlanning(BasePlanning):
    """
    A simple planning module that uses an LLM to generate action outputs.
    """
    
    def __init__(self, model):
        self.llm = model # Pass the initialized model instance

    def generate_output(self, agent, static_world_context, memory_context): 
        """Formats prompt and calls the Gemini API."""

        # Prepare goals string (handle empty list)
        goals_string = "Your current goals:\n" + "\n".join(f"- {g}" for g in agent.goals) if agent.goals else "You have no specific goals right now."

        prompt = f"""You are {agent.name}, a character in a simulated world.
Your personality: {agent.personality}.
Your gender: {getattr(agent, 'gender', 'Not specified')}.
Your goals: {agent.goals}
Your background: {agent.background}

Your current world situation:
{static_world_context}

Your recent memories and perceptions (most recent last):
{memory_context}

Based on your personality, goals, gender, situation, and memories, what do you intend to think, say, or do next?
Choose and describe ONE single intended action, thought, or utterance. You can be descriptive but must focus on only one intent.
If you intend to speak, use quotes. If you intend to think, describe the thought. If you intend to act, describe the action.

IMPORTANT: If someone has spoken to you directly in your recent perceptions, prioritize responding to them before pursuing your own goals. Being responsive to others is crucial for realistic social interaction.

Consider how you might interact with other agents if they're present. You can:
- Talk to them (e.g., "Intend to ask Bob, "Hello, can you help me?"")
- Collaborate with them on tasks
- Observe their behavior
- Respond to their actions or questions
- Form alliances or rivalries based on your goals

Examples of valid single intents:
- Intend to walk towards the Forest Edge to see if I can find any berries.
- Intend to ask Bob, "Did you hear that strange noise coming from the shelter? It sounded like scratching."
- Intend to carefully examine the ground near the shelter for any tracks or clues.
- Intend to think: 'This weather is getting colder. I need to reinforce the shelter soon, especially if Bob plans on staying.'
- Intend to wait silently and observe Bob's next move.
- Intend to respond to Alice, "The forest does look interesting, but I'm more concerned about finding food and water first. What kind of potion are you making?"

Important: Provide only ONE intended action, thought, or utterance. Do not combine multiple intents.
Your intended output (one single intent):"""

        if config.SIMULATION_MODE == 'debug':
            print(f"\n[{agent.name} is thinking...]")
        # print(f"--- DEBUG PROMPT for {agent.name} ---\n{prompt}\n--------------------")
        response=None
        try:
            response = self.llm.generate_content(prompt)
            # Use the full, stripped response text
            utterance = response.text.strip()

            # Check if the response is empty or just whitespace
            if not utterance:
                print(
                    f"[{agent.name} Warning]: LLM gave short/empty response: '{utterance}'. Defaulting to wait intent.")
                utterance = f"Intend to wait silently."
            else:
                if config.SIMULATION_MODE == 'debug':
                    # Print the potentially multi-line intent
                    print(f"[{agent.name} intends]: {utterance}")

            return utterance

        except Exception as e:
            print(f"[{agent.name} Error]: LLM generation failed: {e}")
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:
                print(
                    f"[{agent.name} Safety Block]: Reason: {response.prompt_feedback.block_reason}")
            return f"Intend to pause due to confusion."  # Return an intent


```
============================== END FILE: src_GM\agent\planning.py ==============================
